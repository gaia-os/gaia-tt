{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# InFact: Building Trust in Science through Collaborative Evaluation\n",
    "*A Gaia Lab project*\n",
    "\n",
    "<br>\n",
    "\n",
    "> What we should do is create an institution that collects and evaluates scientific evidence and gives out confidence values based on evidence. -- [Sabine Hossenfelder](https://www.youtube.com/watch?v=zucXnn64qtk&t=314s)\n",
    "\n",
    "The InFact Project is our attempt to realize this vision. We're building a prototype for a decentralized system that evaluates scientific claims and provides a clear measure of confidence based on available evidence.  Imagine a collaborative platform where scientists and the public can work together to assess the reliability of scientific findings, supported by AI and rigorous automated statistics. This is the core idea behind InFact.\n",
    "\n",
    "## How does InFact work?\n",
    "\n",
    "At its heart, InFact uses a network of interconnected nodes. Each node focuses on a specific scientific question, like \"Do human-generated greenhouse gas emissions significantly increase global temperatures?\"\n",
    "\n",
    "Within each node, a sophisticated \"inference engine\" analyzes data related to the question. This engine combines the power of artificial intelligence (specifically, large language models or LLMs) with Bayesian statistics, a mathematical framework for updating beliefs based on evidence.\n",
    "\n",
    "Breaking down the process:\n",
    "\n",
    "* Data collection:  The node gathers data from various sources (research papers, datasets, etc.) related to the scientific question.\n",
    "\n",
    "* AI-powered analysis:  LLMs are used to automatically extract key information from the data, identify relevant studies, and even assess the quality of the evidence.\n",
    "\n",
    "* Bayesian updating:  The system uses Bayesian methods to weigh the evidence and update a \"confidence score\" for the scientific claim. This score reflects the strength of the evidence supporting the claim.\n",
    "\n",
    "* Transparency and traceability:  All data, analyses, and confidence scores are recorded and made available for scrutiny. This ensures transparency and allows for continuous improvement of the system.\n",
    "\n",
    "## Addressing the Challenges of Data Analysis\n",
    "\n",
    "One of the biggest challenges in evaluating scientific claims is the sheer diversity and complexity of scientific data. InFact tackles this challenge by using LLMs to generate custom data analysis pipelines for each new piece of evidence. We use frontier off-the-shelf LLMs (currently, Claude 3.5 Sonnet). These AI models are pre-trained on vast amounts of scientific literature, allowing them to adapt to different types of studies and data formats.\n",
    "\n",
    "## Beyond the Prototype\n",
    "\n",
    "While our current prototype relies heavily on LLMs, we recognize the need for even greater rigor. Our team is developing a framework for \"automatic progressive data analysis.\" This framework will combine the flexibility of LLMs with the reliability of established statistical models, creating a more robust and trustworthy system for evaluating scientific claims.\n",
    "\n",
    "## InFact in Action\n",
    "\n",
    "We envision InFact as a user-friendly platform that presents complex scientific information in a clear and accessible way. Imagine interactive visualizations that show how confidence scores evolve as new evidence emerges, along with explanations that help users understand the reasoning behind the scores.\n",
    "\n",
    "## The Future of Scientific Confidence\n",
    "\n",
    "InFact is more than just a technology; it's a vision for a future where scientific knowledge is more accessible, transparent, and trustworthy. By empowering scientists and the public to collaboratively evaluate evidence, we can foster a deeper understanding of science and its role in shaping our world.\n",
    "\n",
    "## The Gaia Network\n",
    "\n",
    "InFact is also envisioned as a demonstration of the capabilities of the [Gaia Network Protocol](https://gaia-lab.de), the Gaia Lab's main project. Visit our website to learn more.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "gstsQQbg2ILk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Technical description (assumes math/stats background, feel free to skip otherwise)\n",
    "\n",
    "Here we propose **InFact**: a prototype implementation for such a scientific institution, specifically as a decentralized model-based inference engine combining generative AI and Bayesian statistics. InFact is also an implementation of a subset of the [Gaia Network Protocol](https://engineeringideas.substack.com/p/gaia-network-an-illustrated-primer).\n",
    "\n",
    "An InFact node consists of:\n",
    "* A natural-language description of a hypothesis $H$, a binary statement that represents the scope of this node -- i.e., the scientific question the node is supposed to provide confidence values for. Our standing example here will be: \"Human-generated GHG emissions significantly increase global temperatures.\"\n",
    "* A *model* composed of:\n",
    "  * A *likelihood function* that receives a data point $D_i$ and calculates the positive and negative log-likelihoods $l^+_i = \\log P(D_i\\ |\\ H, D_{<i}), l^-_i = \\log P(D_i\\ |\\ \\neg H, D_{<i})$. These are simply the (log) probabilities of such data being observed assuming that $H$ is true and false, respectively.\n",
    "  * A *posterior log-odds ratio* or belief state $\\pi_i = \\log \\frac{P(H\\ |\\ D_1 \\dots D_i)}{P(\\neg H\\ |\\ D_1 \\dots D_i)}$, which is updated by the likelihood function. Before observing any data points, this starts as a *prior log-odds ratio* $\\pi_0$; then after the likelihood function runs, the posterior gets updated as $\\pi_i = \\pi_{i-1} + l^+_i - l^-_i$.\n",
    "  * A set of *hyperparameters* to calculate confidence intervals around the posterior. These can simply be Beta distribution parameters $\\alpha^{+}_i , \\alpha^{-}_i$, updated as $\\alpha^{+}_i = \\alpha^{+}_{i-1} + P(H\\ |\\ D_1 \\dots D_i), \\alpha^{-}_i = \\alpha^{-}_{i-1} + P(\\neg H\\ |\\ D_1 \\dots D_i)$. Assuming that $H$ has a true truth value $p^*$, then this second-order distribution converges to a Delta peaked at $p^*$.\n",
    "* A simple database that stores the sequence of tuples $\\{(D_i, l^+_i, l^-_i, \\pi_i)\\}_i$.\n",
    "\n",
    "At the end we also show a UX for displaying the evolution of $\\pi_i$ and its interpretation.\n",
    "\n",
    "The key challenge here is the model, specifically the likelihood function. Data comes in an immense variety of shapes and sizes; it is often imprecise and noisy. We also often don't have first-hand access to data, only to second-hand reports and summaries. It is impossible to define a likelihood upfront that will adequately and perfectly extract and incorporate all this information into the $l^{+,-}_i$ updates -- which would be equivalent to having a universal data analysis workflow. Instead, we use pre-trained LLMs to perform a plausible approximation of this workflow, effectively generating a custom data analysis pipeline that is updated every time the data set is added to. Contemporary LLMs have sufficient built-in knowledge of science and statistics to perform decently at this task. Furthermore, the LLM prompt and response are recorded, producing a complete rationale which can be inspected and improved upon. (This is sufficient for a prototype, but we can and should aspire for better rigor. Our team is developing a framework for automatic progressive data analysis -- which uses LLMs to propose workflow components, but then crystallizes known-good components into a database of reusable and vetted models, rather than naively relying on LLMs for ab initio analysis every time.)\n",
    "\n",
    "Here is the algorithm we implement:\n",
    "* Receive a data file (ex: a PDF, HTML, CSV, or PNG).\n",
    "* [LLM] Parse the file into a set of data points.\n",
    "* [LLM] Identify which data points are redundant (have been previously observed) and can be safely discarded.\n",
    "* [LLM] Generate a likelihood model for the remaining data points, incorporating all metadata available from the file, as well as background knowledge that the modeler has. Relevant questions: Is this is a meta-analysis or an observational study? Are uncertainties reported, or can they be calculated? How trustworthy is the source?\n",
    "* Run the likelihood model and compute the log-likelihood updates.\n",
    "* Compute the new posterior and Beta hyperparameters.\n",
    "* Store updates in the database.\n",
    "* Render the UX."
   ],
   "metadata": {
    "id": "e0tlAH2Ssbvx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODOs:\n",
    "* Ability to upload data files to a watched directory, triggering an update cycle for the node.\n",
    "* Ability to submit new human-curated/edited models for a data file, triggering an update cycle.\n",
    "* Ability to generate multiple models per data file, ascribe weights to each model, and update the posterior according to a weighted average of the likelihood ratios.\n",
    "* Ability to automatically scrape Web corpora for literature, NotebookLM-style, and upload relevant material as data files.\n",
    "* APIs for submitting data files and models, and for querying the database.\n",
    "* (Advanced) API for counterfactual queries: \"how would the posterior change if we observed/did this\"? This has two possible versions:\n",
    "  * Natural-language queries, turned into statistical queries by the LLM. This is relatively expensive and brittle.\n",
    "  * Queries expressed in a statistical format. These would need to assume a specific model version to make sense, and hence the API also needs to expose a stable data model/random variable ontology, which gets auto-updated whenever the underlying model changes."
   ],
   "metadata": {
    "id": "sHIrAKUy5Q0v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "!pip install anthropic"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "HJyptv6QfIw_",
    "outputId": "b6f88d0b-976e-4ddb-baef-de3ac2b53cb8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: anthropic in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "!pip install autogen"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "UtOuT_NN4IAe",
    "outputId": "da73b815-1b1f-45a3-c2e2-e5e4b0beae3e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: autogen in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
      "Requirement already satisfied: pyautogen==0.6.1 in /usr/local/lib/python3.10/dist-packages (from autogen) (0.6.1)\n",
      "Requirement already satisfied: asyncer>=0.0.8 in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (0.0.8)\n",
      "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (5.6.3)\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (7.1.0)\n",
      "Requirement already satisfied: flaml in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (2.3.3)\n",
      "Requirement already satisfied: openai>=1.58 in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (1.59.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (24.2)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (2.10.3)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (1.0.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (2.5.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (0.8.0)\n",
      "Requirement already satisfied: websockets<15,>=14 in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (14.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from pyautogen==0.6.1->autogen) (1.26.4)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from asyncer>=0.0.8->pyautogen==0.6.1->autogen) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen==0.6.1->autogen) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen==0.6.1->autogen) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen==0.6.1->autogen) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen==0.6.1->autogen) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen==0.6.1->autogen) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen==0.6.1->autogen) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen==0.6.1->autogen) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen==0.6.1->autogen) (2.27.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen==0.6.1->autogen) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen==0.6.1->autogen) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen==0.6.1->autogen) (2024.11.6)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.4.0->asyncer>=0.0.8->pyautogen==0.6.1->autogen) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.4.0->asyncer>=0.0.8->pyautogen==0.6.1->autogen) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.6.1->autogen) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.6.1->autogen) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.6.1->autogen) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen==0.6.1->autogen) (3.4.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "import anthropic\n",
    "from typing import Any, Dict, Union, List, Tuple\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from autogen.code_utils import extract_code"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "wKG7XG5KfFjW",
    "outputId": "f385e6f6-9744-4861-fb87-639002cc032b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "class AnthropicInFactNode:\n",
    "    def __init__(self,\n",
    "                hypothesis: str,\n",
    "                api_key: str,\n",
    "                prior_log_odds: float = 0.0,\n",
    "                log_level: int = logging.INFO):\n",
    "        \"\"\"Initialize node with logging configuration.\"\"\"\n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Create a unique log file for this instance\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_dir = Path(\"logs\")\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        file_handler = logging.FileHandler(\n",
    "            log_dir / f\"infact_{timestamp}.log\",\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        file_handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "        # Initialize API client\n",
    "        self.api_key = api_key\n",
    "        self.logger.info(f\"Initializing AnthropicInFactNode with hypothesis: {hypothesis}\")\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "        # Store parameters\n",
    "        self.hypothesis = hypothesis\n",
    "        self.prior_log_odds = prior_log_odds\n",
    "        self.current_posterior = prior_log_odds\n",
    "        self.data_points = []\n",
    "\n",
    "        self.logger.info(\"Initialization complete\")\n",
    "\n",
    "    def save(self, filename: str):\n",
    "        \"\"\"Save node's data to a JSON file.\"\"\"\n",
    "        data = {\n",
    "            'hypothesis': self.hypothesis,\n",
    "            'prior_log_odds': self.prior_log_odds,\n",
    "            'current_posterior': self.current_posterior,\n",
    "            'data_points': [\n",
    "                {\n",
    "                    'metadata': dp['metadata'],\n",
    "                    'raw_data': dp['raw_data'],\n",
    "                    'l_plus': dp['l_plus'],\n",
    "                    'l_minus': dp['l_minus'],\n",
    "                    'posterior': dp['posterior'],\n",
    "                    'confidence_assessment': dp.get('confidence_assessment', {}),\n",
    "                    'analysis_rationale': dp.get('analysis_rationale', '')\n",
    "                }\n",
    "                for dp in self.data_points\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self.logger.info(f\"Saving node data to {filename}\")\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename: str, api_key: str = None):\n",
    "        \"\"\"Load node from a JSON file.\"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Create new node\n",
    "        node = cls(\n",
    "            hypothesis=data['hypothesis'],\n",
    "            api_key=api_key,\n",
    "            prior_log_odds=data['prior_log_odds']\n",
    "        )\n",
    "\n",
    "        # Restore state\n",
    "        node.current_posterior = data['current_posterior']\n",
    "        node.data_points = data['data_points']\n",
    "\n",
    "        node.logger.info(f\"Loaded node data from {filename}\")\n",
    "        return node\n",
    "\n",
    "    def process_data(self, data_file: str) -> Tuple[float, Tuple[float, float]]:\n",
    "        \"\"\"Process a new data file and update beliefs.\"\"\"\n",
    "        self.logger.info(f\"Processing data file: {data_file}\")\n",
    "\n",
    "        try:\n",
    "            # Parse data\n",
    "            parsed_data = self._parse_data(data_file)\n",
    "            self.logger.debug(f\"Parsed data: {json.dumps(parsed_data, indent=2)}\")\n",
    "\n",
    "            # Check redundancy\n",
    "            if self._is_redundant(parsed_data):\n",
    "                self.logger.info(\"Data determined to be redundant, skipping\")\n",
    "                return self.current_posterior, self._calculate_uncertainty()\n",
    "\n",
    "            # Analyze data\n",
    "            l_plus, l_minus, code = self._analyze_data(parsed_data)\n",
    "            self.logger.info(f\"Analysis results - l_plus: {l_plus}, l_minus: {l_minus}\")\n",
    "\n",
    "            # Update posterior\n",
    "            new_posterior = self.current_posterior + l_plus - l_minus\n",
    "            self.logger.info(f\"Updated posterior from {self.current_posterior} to {new_posterior}\")\n",
    "\n",
    "            # Store data point\n",
    "            self.data_points.append({\n",
    "                'raw_data': parsed_data,\n",
    "                'metadata': self._extract_metadata(data_file),\n",
    "                'l_plus': l_plus,\n",
    "                'l_minus': l_minus,\n",
    "                'posterior': new_posterior,\n",
    "                'confidence_assessment': parsed_data.get('confidence_assessment', {\n",
    "                    'confidence_score': 0,\n",
    "                    'explanation': 'No confidence assessment available',\n",
    "                    'key_strengths': [],\n",
    "                    'key_limitations': []\n",
    "                }),\n",
    "                'analysis_rationale': code  # Store the analysis code used\n",
    "            })\n",
    "\n",
    "            self.current_posterior = new_posterior\n",
    "            lower, upper = self._calculate_uncertainty()\n",
    "\n",
    "            self.logger.info(f\"Processing complete. Current probability: {self._to_probability(new_posterior):.2%} ({lower:.2%}, {upper:.2%})\")\n",
    "            return new_posterior, (lower, upper)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {data_file}: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def _parse_data(self, data_file: str) -> Dict:\n",
    "        \"\"\"Parse different file types using LLM assistance.\"\"\"\n",
    "        self.logger.info(f\"Parsing data file: {data_file}\")\n",
    "        file_type = Path(data_file).suffix.lower()\n",
    "\n",
    "        try:\n",
    "            if file_type == '.csv':\n",
    "                self.logger.debug(\"Processing CSV file\")\n",
    "                df = pd.read_csv(data_file)\n",
    "                content = df.to_string()\n",
    "                message_content = [{\"type\": \"text\", \"text\": content}]\n",
    "\n",
    "            elif file_type in ['.pdf', '.PDF']:\n",
    "                self.logger.debug(\"Processing PDF file\")\n",
    "                with open(data_file, 'rb') as f:\n",
    "                    pdf_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                message_content = [\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"application/pdf\",\n",
    "                            \"data\": pdf_data\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            elif file_type in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:\n",
    "                self.logger.debug(f\"Processing image file of type {file_type}\")\n",
    "                with open(data_file, 'rb') as f:\n",
    "                    img_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                media_type = {\n",
    "                    '.png': 'image/png',\n",
    "                    '.jpg': 'image/jpeg',\n",
    "                    '.jpeg': 'image/jpeg',\n",
    "                    '.gif': 'image/gif',\n",
    "                    '.webp': 'image/webp'\n",
    "                }[file_type]\n",
    "                message_content = [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": media_type,\n",
    "                            \"data\": img_data\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                self.logger.debug(f\"Processing text file of type {file_type}\")\n",
    "                with open(data_file, 'r') as f:\n",
    "                    content = f.read()\n",
    "                message_content = [{\"type\": \"text\", \"text\": content}]\n",
    "\n",
    "            # Add analysis prompt\n",
    "            prompt = f\"\"\"\n",
    "            Extract relevant data points for evaluating the hypothesis:\n",
    "            \"{self.hypothesis}\"\n",
    "\n",
    "            Provide your response as a JSON code block, like this:\n",
    "            ```json\n",
    "            {{\n",
    "                \"numerical_values\": [],\n",
    "                \"metadata\": {{}},\n",
    "                \"issues\": [],\n",
    "                \"confidence_assessment\": {{\n",
    "                    \"confidence_score\": 0.75,\n",
    "                    \"explanation\": \"Detailed explanation of confidence level\",\n",
    "                    \"key_strengths\": [\n",
    "                        \"Strength 1\",\n",
    "                        \"Strength 2\"\n",
    "                    ],\n",
    "                    \"key_limitations\": [\n",
    "                        \"Limitation 1\",\n",
    "                        \"Limitation 2\"\n",
    "                    ]\n",
    "                }}\n",
    "            }}\n",
    "            ```\n",
    "\n",
    "            The confidence_assessment should:\n",
    "            1. Include a confidence_score between 0 and 1\n",
    "            2. Provide a detailed explanation of the confidence level\n",
    "            3. List key strengths of the evidence\n",
    "            4. List key limitations or potential issues\n",
    "\n",
    "            The overall JSON should include:\n",
    "            1. Extracted numerical values and their uncertainties\n",
    "            2. Relevant metadata (source quality, methodology, etc.)\n",
    "            3. Any potential issues or biases in the data\n",
    "            \"\"\"\n",
    "\n",
    "            message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "            self.logger.debug(f\"Prepared prompt: {prompt}\")\n",
    "\n",
    "            # Send to Anthropic API\n",
    "            self.logger.info(\"Sending request to Anthropic API\")\n",
    "            message = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8192,\n",
    "                temperature=0.1,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": message_content\n",
    "                }]\n",
    "            )\n",
    "\n",
    "            # Extract and parse response\n",
    "            response_text = self._get_message_text(message)\n",
    "            self.logger.debug(f\"Received API response: {response_text}\")\n",
    "\n",
    "            # Try to extract JSON using autogen\n",
    "            extracted_blocks = extract_code(response_text)\n",
    "\n",
    "            # Look for JSON blocks\n",
    "            json_str = None\n",
    "            for lang, block in extracted_blocks:\n",
    "                if lang.lower() in ['json', '']:\n",
    "                    try:\n",
    "                        # Try to parse as JSON to validate\n",
    "                        parsed = json.loads(block)\n",
    "                        json_str = block\n",
    "                        break\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "            # If no valid JSON block found, try parsing the whole response\n",
    "            if not json_str:\n",
    "                self.logger.warning(\"No JSON code block found, trying to parse entire response\")\n",
    "                try:\n",
    "                    parsed = json.loads(response_text)\n",
    "                    json_str = response_text\n",
    "                except json.JSONDecodeError:\n",
    "                    self.logger.error(\"Failed to parse response as JSON\")\n",
    "                    return {\n",
    "                        \"extraction_error\": \"Failed to parse LLM response\",\n",
    "                        \"raw_response\": response_text\n",
    "                    }\n",
    "\n",
    "            parsed_data = json.loads(json_str)\n",
    "            self.logger.debug(f\"Successfully parsed JSON data: {json.dumps(parsed_data, indent=2)}\")\n",
    "            return parsed_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in _parse_data: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def _extract_metadata(self, data_file: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract metadata from the data file.\"\"\"\n",
    "        self.logger.info(f\"Extracting metadata from {data_file}\")\n",
    "\n",
    "        try:\n",
    "            file_path = Path(data_file)\n",
    "            basic_metadata = {\n",
    "                \"filename\": file_path.name,\n",
    "                \"file_type\": file_path.suffix.lower(),\n",
    "                \"file_size\": file_path.stat().st_size,\n",
    "                \"last_modified\": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),\n",
    "                \"source_path\": str(file_path.absolute())\n",
    "            }\n",
    "\n",
    "            if file_path.suffix.lower() == '.pdf':\n",
    "                try:\n",
    "                    import PyPDF2\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        pdf = PyPDF2.PdfReader(f)\n",
    "                        if pdf.metadata:\n",
    "                            basic_metadata.update({\n",
    "                                \"title\": pdf.metadata.get('/Title', ''),\n",
    "                                \"author\": pdf.metadata.get('/Author', ''),\n",
    "                                \"creator\": pdf.metadata.get('/Creator', ''),\n",
    "                                \"producer\": pdf.metadata.get('/Producer', ''),\n",
    "                                \"creation_date\": pdf.metadata.get('/CreationDate', ''),\n",
    "                                \"modification_date\": pdf.metadata.get('/ModDate', ''),\n",
    "                                \"page_count\": len(pdf.pages)\n",
    "                            })\n",
    "                except ImportError:\n",
    "                    self.logger.warning(\"PyPDF2 not installed, skipping PDF metadata extraction\")\n",
    "\n",
    "            elif file_path.suffix.lower() in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:\n",
    "                try:\n",
    "                    from PIL import Image\n",
    "                    with Image.open(file_path) as img:\n",
    "                        basic_metadata.update({\n",
    "                            \"image_format\": img.format,\n",
    "                            \"image_size\": img.size,\n",
    "                            \"image_mode\": img.mode,\n",
    "                            \"image_info\": dict(img.info)\n",
    "                        })\n",
    "                except ImportError:\n",
    "                    self.logger.warning(\"Pillow not installed, skipping image metadata extraction\")\n",
    "\n",
    "            elif file_path.suffix.lower() == '.html':\n",
    "                try:\n",
    "                    from bs4 import BeautifulSoup\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "                        meta_tags = {}\n",
    "                        for meta in soup.find_all('meta'):\n",
    "                            name = meta.get('name', meta.get('property', ''))\n",
    "                            content = meta.get('content', '')\n",
    "                            if name and content:\n",
    "                                meta_tags[name] = content\n",
    "\n",
    "                        basic_metadata.update({\n",
    "                            \"title\": soup.title.string if soup.title else '',\n",
    "                            \"meta_tags\": meta_tags,\n",
    "                            \"has_article\": bool(soup.find('article')),\n",
    "                            \"has_main\": bool(soup.find('main')),\n",
    "                            \"num_headers\": len(soup.find_all(['h1', 'h2', 'h3'])),\n",
    "                            \"has_tables\": bool(soup.find_all('table'))\n",
    "                        })\n",
    "                except ImportError:\n",
    "                    self.logger.warning(\"BeautifulSoup4 not installed, skipping HTML metadata extraction\")\n",
    "\n",
    "            elif file_path.suffix.lower() == '.csv':\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    basic_metadata.update({\n",
    "                        \"num_rows\": len(df),\n",
    "                        \"num_columns\": len(df.columns),\n",
    "                        \"column_names\": list(df.columns),\n",
    "                        \"data_types\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "                        \"has_nulls\": df.isnull().any().any()\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error extracting CSV metadata: {str(e)}\")\n",
    "\n",
    "            self.logger.debug(f\"Extracted metadata: {json.dumps(basic_metadata, indent=2)}\")\n",
    "            return basic_metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in _extract_metadata: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"filename\": data_file,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def _analyze_data(self, data: Dict) -> Tuple[float, float, str]:\n",
    "        \"\"\"Generate and execute analysis code using LLM.\"\"\"\n",
    "        self.logger.info(\"Analyzing parsed data\")\n",
    "\n",
    "        MAX_LOG_LIKELIHOOD_RATIO = 5.\n",
    "\n",
    "        try:\n",
    "            # Generate analysis code\n",
    "            prompt = f\"\"\"\n",
    "            Given this data:\n",
    "            {json.dumps(data, indent=2)}\n",
    "\n",
    "            Generate Python code to calculate log likelihoods for the hypothesis:\n",
    "            \"{self.hypothesis}\"\n",
    "\n",
    "            This should be a single function named `calculate_log_likelihoods`.\n",
    "            It should take a single argument, a dict with the format given above,\n",
    "            and output only the tuple of log-likelihoods\n",
    "              l_plus = log P(data | hypothesis),\n",
    "              l_minus = log P(data | not hypothesis).\n",
    "\n",
    "            The code should:\n",
    "            1. Calculate l_plus and l_minus (log likelihoods)\n",
    "            2. Handle uncertainties properly\n",
    "            3. Account for data quality and potential biases\n",
    "            4. Limit overconfidence by capping the absolute difference between l_plus and l_minus to {MAX_LOG_LIKELIHOOD_RATIO}.\n",
    "            4. Use the usual libraries such as numpy and scipy for calculations\n",
    "            5. Use print() to output intermediate results, as well as the final result before returning.\n",
    "\n",
    "            Return only executable Python code with the function definition.\n",
    "            Do not include the function call itself.\n",
    "            \"\"\"\n",
    "\n",
    "            self.logger.debug(f\"Analysis prompt: {prompt}\")\n",
    "\n",
    "            message = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8192,\n",
    "                temperature=0.1,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "\n",
    "            response_text = self._get_message_text(message)\n",
    "            self.logger.debug(f\"API response: {response_text}\")\n",
    "\n",
    "            # Extract code using autogen\n",
    "            from autogen.code_utils import extract_code\n",
    "            extracted_code = extract_code(response_text)\n",
    "\n",
    "            if not extracted_code:\n",
    "                self.logger.error(\"No code block found in API response\")\n",
    "                raise\n",
    "\n",
    "            # Get the first Python code block\n",
    "            code = None\n",
    "            for lang, code_block in extracted_code:\n",
    "                if lang.lower() in ['python', 'py', '']:\n",
    "                    code = code_block\n",
    "                    break\n",
    "\n",
    "            if not code:\n",
    "                self.logger.error(\"No Python code block found in API response\")\n",
    "                raise\n",
    "\n",
    "            self.logger.debug(f\"Extracted Python code: {code}\")\n",
    "\n",
    "            # Execute the code\n",
    "            l_plus, l_minus, code = self._execute_code_with_debug(code, data)\n",
    "            return l_plus, l_minus, code\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in _analyze_data: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def _execute_code_with_debug(self, code: str, data: Dict, max_attempts: int = 5) -> Tuple[float, float, str]:\n",
    "        \"\"\"Execute code with debug loop for error correction.\"\"\"\n",
    "        globals_dict = {\n",
    "            \"np\": np,\n",
    "            \"math\": math,\n",
    "            \"data\": data\n",
    "        }\n",
    "\n",
    "        attempt = 1\n",
    "        while attempt <= max_attempts:\n",
    "            self.logger.info(f\"Code execution attempt {attempt}/{max_attempts}\")\n",
    "            self.logger.debug(f\"Executing code:\\n{code}\")\n",
    "\n",
    "            try:\n",
    "                exec(code, globals_dict)\n",
    "                l_plus, l_minus = globals_dict['calculate_log_likelihoods'](data)\n",
    "\n",
    "                # Validate outputs\n",
    "                if l_plus is None or l_minus is None:\n",
    "                    raise ValueError(\"Code did not define l_plus and l_minus\")\n",
    "\n",
    "                if not (isinstance(l_plus, (int, float)) and isinstance(l_minus, (int, float))):\n",
    "                    raise ValueError(\"l_plus and l_minus must be numeric values\")\n",
    "\n",
    "                self.logger.info(f\"Code execution successful - l_plus: {l_plus}, l_minus: {l_minus}\")\n",
    "                return float(l_plus), float(l_minus), code\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Code execution failed on attempt {attempt}: {str(e)}\")\n",
    "\n",
    "                if attempt == max_attempts:\n",
    "                    self.logger.error(\"Max attempts reached, raising error\")\n",
    "                    raise RuntimeError(f\"Failed to generate working code after {max_attempts} attempts. Final error: {str(e)}\")\n",
    "\n",
    "                # Ask LLM to fix the code\n",
    "                debug_prompt = f\"\"\"\n",
    "                The following code failed with error: {str(e)}\n",
    "\n",
    "                Code:\n",
    "                ```python\n",
    "                {code}\n",
    "                ```\n",
    "\n",
    "                Input data:\n",
    "                ```json\n",
    "                {json.dumps(data, indent=2)}\n",
    "                ```\n",
    "\n",
    "                Please fix the code to:\n",
    "                1. Handle the error properly\n",
    "                2. Return numeric values for l_plus and l_minus\n",
    "                3. Include proper error checking\n",
    "                4. Handle edge cases in the input data\n",
    "\n",
    "                Return only the corrected Python code.\n",
    "                \"\"\"\n",
    "\n",
    "                self.logger.debug(f\"Sending debug prompt to LLM:\\n{debug_prompt}\")\n",
    "\n",
    "                message = self.client.messages.create(\n",
    "                    model=\"claude-3-5-sonnet-20241022\",\n",
    "                    max_tokens=8192,\n",
    "                    temperature=0.1,\n",
    "                    messages=[{\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": debug_prompt\n",
    "                    }]\n",
    "                )\n",
    "\n",
    "                # Extract corrected code\n",
    "                response_text = self._get_message_text(message)\n",
    "                extracted_code = extract_code(response_text)\n",
    "\n",
    "                if not extracted_code:\n",
    "                    self.logger.error(\"No code block found in debug response\")\n",
    "                    attempt += 1\n",
    "                    continue\n",
    "\n",
    "                # Get the first Python code block\n",
    "                for lang, code_block in extracted_code:\n",
    "                    if lang.lower() in ['python', 'py', '']:\n",
    "                        code = code_block\n",
    "                        break\n",
    "                else:\n",
    "                    self.logger.error(\"No Python code block found in debug response\")\n",
    "                    attempt += 1\n",
    "                    continue\n",
    "\n",
    "            attempt += 1\n",
    "\n",
    "        # Should never reach here due to raise in loop\n",
    "        raise RuntimeError(\"Unexpected error in debug loop\")\n",
    "\n",
    "    def _is_redundant(self, new_data: Dict) -> bool:\n",
    "        \"\"\"Check if new data is redundant with existing data.\"\"\"\n",
    "        self.logger.info(\"Checking for data redundancy\")\n",
    "\n",
    "        if not self.data_points:\n",
    "            self.logger.debug(\"No existing data points, not redundant\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Compare the following new data:\n",
    "            {json.dumps(new_data, indent=2)}\n",
    "\n",
    "            With these existing data points:\n",
    "            {json.dumps([dp['raw_data'] for dp in self.data_points], indent=2)}\n",
    "\n",
    "            Is the new data redundant with any existing data points?\n",
    "            Consider:\n",
    "            1. Same source or study being cited\n",
    "            2. Same measurements within uncertainty\n",
    "            3. Derived results from already incorporated primary data\n",
    "\n",
    "            Return \"true\" if redundant, \"false\" if novel information.\n",
    "            \"\"\"\n",
    "\n",
    "            self.logger.debug(f\"Redundancy check prompt: {prompt}\")\n",
    "\n",
    "            message = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=4096,\n",
    "                temperature=0.1,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "\n",
    "            response = self._get_message_text(message)\n",
    "            self.logger.debug(f\"Redundancy check response: {response}\")\n",
    "\n",
    "            is_redundant = response.strip().lower() == \"true\"\n",
    "            self.logger.info(f\"Redundancy check result: {is_redundant}\")\n",
    "            return is_redundant\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in _is_redundant: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def _get_message_text(self, message) -> str:\n",
    "        \"\"\"Extract text content from message response.\"\"\"\n",
    "        if message.content and len(message.content) > 0:\n",
    "            content_block = message.content[0]\n",
    "            if hasattr(content_block, 'text'):\n",
    "                return content_block.text\n",
    "        return \"\"\n",
    "\n",
    "    def _calculate_uncertainty(self) -> tuple[float, float]:\n",
    "        \"\"\"Calculate 95% confidence interval for the posterior probability.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: Lower and upper bounds of the 95% CI\n",
    "        \"\"\"\n",
    "        # Convert current posterior log-odds to probability\n",
    "        p = self._to_probability(self.current_posterior)\n",
    "\n",
    "        # Calculate total weight of evidence from Bayes factors\n",
    "        total_evidence = sum(\n",
    "            abs(math.exp(dp['l_plus'] - dp['l_minus']) - 1)\n",
    "            for dp in self.data_points\n",
    "        )\n",
    "\n",
    "        if total_evidence < 1e-6:\n",
    "            return (0.0, 1.0)  # Default CI for effectively no data\n",
    "\n",
    "        # Each Bayes factor represents the weight of evidence\n",
    "        # The concentration parameter of our Beta should reflect this\n",
    "        concentration = total_evidence\n",
    "\n",
    "        # Calculate Beta parameters to maintain the mean at p\n",
    "        alpha = concentration * p\n",
    "        beta = concentration * (1 - p)\n",
    "\n",
    "        # Calculate 95% confidence interval\n",
    "        from scipy import stats\n",
    "        ci_low, ci_high = stats.beta.interval(0.95, alpha, beta)\n",
    "\n",
    "        # Clip to [0, 1]\n",
    "        ci_low = max(0.0, min(1.0, ci_low))\n",
    "        ci_high = max(0.0, min(1.0, ci_high))\n",
    "\n",
    "        return ci_low, ci_high\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_probability(log_odds: float) -> float:\n",
    "        \"\"\"Convert log odds to probability.\"\"\"\n",
    "        return 1 / (1 + np.exp(-log_odds))"
   ],
   "metadata": {
    "cellView": "form",
    "id": "QLL70uldfocA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BbgU8wZGrJqo"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class DataPoint:\n",
    "    raw_data: str  # Original data representation\n",
    "    metadata: dict  # Source information, timestamps, etc.\n",
    "    likelihood_plus: float  # l^+_i\n",
    "    likelihood_minus: float  # l^-_i\n",
    "    posterior: float  # _i\n",
    "    analysis_rationale: str  # LLM explanation for likelihood calculation\n",
    "\n",
    "@dataclass\n",
    "class HyperParameters:\n",
    "    alpha_plus: float  # ^+\n",
    "    alpha_minus: float  # ^-\n",
    "\n",
    "    def update(self, p_h: float):\n",
    "        \"\"\"Update hyperparameters based on new posterior probability\"\"\"\n",
    "        self.alpha_plus += p_h\n",
    "        self.alpha_minus += (1 - p_h)\n",
    "\n",
    "    def confidence_interval(self, confidence: float = 0.95) -> Tuple[float, float]:\n",
    "        \"\"\"Calculate confidence interval for the belief state\"\"\"\n",
    "        from scipy import stats\n",
    "        a = self.alpha_plus\n",
    "        b = self.alpha_minus\n",
    "        interval = stats.beta.interval(confidence, a, b)\n",
    "        return interval\n",
    "\n",
    "class InFactNode:\n",
    "    def __init__(self, hypothesis: str, prior_log_odds: float = 0.0):\n",
    "        self.hypothesis = hypothesis\n",
    "        self.prior_log_odds = prior_log_odds\n",
    "        self.current_posterior = prior_log_odds\n",
    "        self.hyperparameters = HyperParameters(1.0, 1.0)  # Start with uniform Beta\n",
    "        self.data_points: List[DataPoint] = []\n",
    "\n",
    "    def process_data(self, data_file: str) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Process a new data file and update beliefs.\n",
    "        Returns the updated posterior and its uncertainty.\n",
    "        \"\"\"\n",
    "        # Parse data file (implemented by derived classes for specific file types)\n",
    "        data = self._parse_data(data_file)\n",
    "\n",
    "        # Check for redundancy with existing data points\n",
    "        if self._is_redundant(data):\n",
    "            return self.current_posterior, self._calculate_uncertainty()\n",
    "\n",
    "        # Generate and run analysis code (implemented by derived classes)\n",
    "        l_plus, l_minus = self._analyze_data(data)\n",
    "\n",
    "        # Update posterior\n",
    "        new_posterior = self.current_posterior + l_plus - l_minus\n",
    "\n",
    "        # Update hyperparameters\n",
    "        p_h = math.exp(new_posterior) / (1 + math.exp(new_posterior))  # Convert log-odds to probability\n",
    "        self.hyperparameters.update(p_h)\n",
    "\n",
    "        # Store data point\n",
    "        data_point = DataPoint(\n",
    "            raw_data=str(data),\n",
    "            metadata=self._extract_metadata(data_file),\n",
    "            likelihood_plus=l_plus,\n",
    "            likelihood_minus=l_minus,\n",
    "            posterior=new_posterior,\n",
    "            analysis_rationale=\"\"  # Filled by derived class\n",
    "        )\n",
    "        self.data_points.append(data_point)\n",
    "\n",
    "        self.current_posterior = new_posterior\n",
    "        return new_posterior, self._calculate_uncertainty()\n",
    "\n",
    "    def _calculate_uncertainty(self) -> float:\n",
    "        \"\"\"Calculate confidence interval width for the posterior probability.\"\"\"\n",
    "        # Convert current posterior log-odds to probability\n",
    "        p = self._to_probability(self.current_posterior)\n",
    "\n",
    "        # Use number of data points to determine effective sample size\n",
    "        n = len(self.data_points)\n",
    "        if n < 1:\n",
    "            return 0.1  # Default uncertainty for no data\n",
    "\n",
    "        # Calculate Beta distribution parameters\n",
    "        # Start with uniform prior (==1)\n",
    "        alpha = 1 + n * p\n",
    "        beta = 1 + n * (1 - p)\n",
    "\n",
    "        # Calculate 95% confidence interval\n",
    "        from scipy import stats\n",
    "        ci_low, ci_high = stats.beta.interval(0.95, alpha, beta)\n",
    "\n",
    "        # Return half the interval width as the  uncertainty\n",
    "        return (ci_high - ci_low) / 2\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_probability(log_odds: float) -> float:\n",
    "        \"\"\"Convert log odds to probability.\"\"\"\n",
    "        return 1 / (1 + np.exp(-log_odds))\n",
    "\n",
    "    def _is_redundant(self, new_data) -> bool:\n",
    "        \"\"\"Check if new data has already been incorporated\"\"\"\n",
    "        # Basic implementation - should be overridden with more sophisticated redundancy detection\n",
    "        return str(new_data) in [dp.raw_data for dp in self.data_points]\n",
    "\n",
    "    def _extract_metadata(self, data_file: str) -> dict:\n",
    "        \"\"\"Extract metadata from data file\"\"\"\n",
    "        # Basic implementation - should be overridden for specific file types\n",
    "        return {\n",
    "            \"filename\": data_file,\n",
    "            \"timestamp\": \"\",  # Fill with actual timestamp\n",
    "            \"format\": data_file.split(\".\")[-1]\n",
    "        }\n",
    "\n",
    "    def get_belief_history(self) -> List[Dict]:\n",
    "        \"\"\"Get history of belief updates for visualization\"\"\"\n",
    "        history = []\n",
    "        current = self.prior_log_odds\n",
    "        for dp in self.data_points:\n",
    "            current += dp.likelihood_plus - dp.likelihood_minus\n",
    "            history.append({\n",
    "                \"posterior\": current,\n",
    "                \"probability\": math.exp(current) / (1 + math.exp(current)),\n",
    "                \"data_point\": dp.raw_data,\n",
    "                \"rationale\": dp.analysis_rationale\n",
    "            })\n",
    "        return history\n",
    "\n",
    "    def save_state(self, filename: str):\n",
    "        \"\"\"Save current state to file\"\"\"\n",
    "        state = {\n",
    "            \"hypothesis\": self.hypothesis,\n",
    "            \"prior_log_odds\": self.prior_log_odds,\n",
    "            \"current_posterior\": self.current_posterior,\n",
    "            \"hyperparameters\": {\n",
    "                \"alpha_plus\": self.hyperparameters.alpha_plus,\n",
    "                \"alpha_minus\": self.hyperparameters.alpha_minus\n",
    "            },\n",
    "            \"data_points\": [vars(dp) for dp in self.data_points]\n",
    "        }\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "\n",
    "    def load_state(self, filename: str):\n",
    "        \"\"\"Load state from file\"\"\"\n",
    "        with open(filename, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "\n",
    "        self.hypothesis = state[\"hypothesis\"]\n",
    "        self.prior_log_odds = state[\"prior_log_odds\"]\n",
    "        self.current_posterior = state[\"current_posterior\"]\n",
    "        self.hyperparameters = HyperParameters(\n",
    "            state[\"hyperparameters\"][\"alpha_plus\"],\n",
    "            state[\"hyperparameters\"][\"alpha_minus\"]\n",
    "        )\n",
    "        self.data_points = [DataPoint(**dp) for dp in state[\"data_points\"]]\n",
    "\n",
    "    # Abstract methods to be implemented by derived classes\n",
    "    def _parse_data(self, data_file: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _analyze_data(self, data) -> Tuple[float, float]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>InFact Analysis: {{ hypothesis }}</title>\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: system-ui, -apple-system, sans-serif;\n",
    "            line-height: 1.5;\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            padding: 2rem;\n",
    "            color: #1a1a1a;\n",
    "        }\n",
    "\n",
    "        .card {\n",
    "            background: white;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
    "            padding: 1.5rem;\n",
    "            margin-bottom: 1.5rem;\n",
    "        }\n",
    "\n",
    "        .hypothesis {\n",
    "            font-size: 1.2rem;\n",
    "            color: #4a5568;\n",
    "            margin: 1rem 0;\n",
    "            padding: 1rem;\n",
    "            background: #f7fafc;\n",
    "            border-radius: 6px;\n",
    "        }\n",
    "\n",
    "        .final-assessment {\n",
    "            text-align: center;\n",
    "            padding: 2rem;\n",
    "            background: #ebf8ff;\n",
    "        }\n",
    "\n",
    "        .probability {\n",
    "            font-size: 3rem;\n",
    "            font-weight: bold;\n",
    "            color: #2b6cb0;\n",
    "        }\n",
    "\n",
    "        .uncertainty {\n",
    "            font-size: 1.2rem;\n",
    "            color: #4a5568;\n",
    "            margin-top: 0.5rem;\n",
    "        }\n",
    "\n",
    "        .evidence-point {\n",
    "            border-left: 4px solid #4299e1;\n",
    "            padding-left: 1rem;\n",
    "            margin-bottom: 2rem;\n",
    "        }\n",
    "\n",
    "        .evidence-grid {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n",
    "            gap: 1rem;\n",
    "        }\n",
    "\n",
    "        .stats-grid {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(3, 1fr);\n",
    "            gap: 1rem;\n",
    "            margin: 1rem 0;\n",
    "        }\n",
    "\n",
    "        .stat-card {\n",
    "            background: #f7fafc;\n",
    "            padding: 1rem;\n",
    "            border-radius: 6px;\n",
    "            text-align: center;\n",
    "        }\n",
    "\n",
    "        .stat-value {\n",
    "            font-size: 1.5rem;\n",
    "            font-weight: bold;\n",
    "            color: #2b6cb0;\n",
    "        }\n",
    "\n",
    "        .stat-label {\n",
    "            font-size: 0.875rem;\n",
    "            color: #4a5568;\n",
    "            margin-top: 0.25rem;\n",
    "        }\n",
    "\n",
    "        .chart-container {\n",
    "            height: 400px;\n",
    "            margin: 2rem 0;\n",
    "        }\n",
    "\n",
    "        .confidence-high { color: #047857; }\n",
    "        .confidence-medium { color: #b45309; }\n",
    "        .confidence-low { color: #dc2626; }\n",
    "\n",
    "        details {\n",
    "            margin-top: 1rem;\n",
    "        }\n",
    "\n",
    "        summary {\n",
    "            cursor: pointer;\n",
    "            color: #2b6cb0;\n",
    "            font-weight: 500;\n",
    "        }\n",
    "\n",
    "        pre {\n",
    "            background: #f7fafc;\n",
    "            padding: 1rem;\n",
    "            border-radius: 6px;\n",
    "            overflow-x: auto;\n",
    "            font-size: 0.875rem;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>Evidence Analysis</h1>\n",
    "        <div class=\"hypothesis\">{{ hypothesis }}</div>\n",
    "    </header>\n",
    "\n",
    "    <main>\n",
    "        <section class=\"card final-assessment\">\n",
    "            <h2>Current Assessment</h2>\n",
    "            <div class=\"probability\">\n",
    "                {{ \"%.1f\"|format(final_probability * 100) }}%\n",
    "            </div>\n",
    "            <div class=\"uncertainty\">\n",
    "                ({{ \"%.1f\"|format(ci_low * 100) }}%, {{ \"%.1f\"|format(ci_high * 100) }}%)\n",
    "            </div>\n",
    "            <div class=\"interpretation\">\n",
    "                {% if final_probability > 0.99 %}Virtually Certain\n",
    "                {% elif final_probability > 0.95 %}Extremely Likely\n",
    "                {% elif final_probability > 0.90 %}Very Likely\n",
    "                {% elif final_probability > 0.66 %}Likely\n",
    "                {% elif final_probability > 0.33 %}Uncertain\n",
    "                {% elif final_probability > 0.10 %}Unlikely\n",
    "                {% elif final_probability > 0.05 %}Very Unlikely\n",
    "                {% elif final_probability > 0.01 %}Extremely Unlikely\n",
    "                {% else %}Virtually Impossible{% endif %}\n",
    "            </div>\n",
    "        </section>\n",
    "\n",
    "        <section class=\"card\">\n",
    "            <h2>Belief Evolution</h2>\n",
    "            <div class=\"chart-container\">\n",
    "                <canvas id=\"beliefChart\"></canvas>\n",
    "            </div>\n",
    "        </section>\n",
    "\n",
    "        <section class=\"card\">\n",
    "            <h2>Evidence Analysis</h2>\n",
    "            {% for point in evidence_points %}\n",
    "            <div class=\"evidence-point\">\n",
    "                <h3>Evidence {{ loop.index }}: {{ point.file }}</h3>\n",
    "\n",
    "                <div class=\"stats-grid\">\n",
    "                    <div class=\"stat-card\">\n",
    "                        <div class=\"stat-value\">{{ \"%.1f\"|format(point.prior_prob * 100) }}%</div>\n",
    "                        <div class=\"stat-label\">Prior Probability</div>\n",
    "                    </div>\n",
    "                    <div class=\"stat-card\">\n",
    "                        <div class=\"stat-value\">{{ \"%.1f\"|format(point.likelihood_ratio) }}</div>\n",
    "                        <div class=\"stat-label\">Likelihood Ratio</div>\n",
    "                    </div>\n",
    "                    <div class=\"stat-card\">\n",
    "                        <div class=\"stat-value\">{{ \"%.1f\"|format(point.posterior * 100) }}%</div>\n",
    "                        <div class=\"stat-label\">Posterior Probability</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "\n",
    "                <div class=\"evidence-grid\">\n",
    "                    <div>\n",
    "                        <h4>Confidence Assessment</h4>\n",
    "                        <div class=\"confidence-score\n",
    "                            {% if point.confidence_assessment.confidence_score > 0.7 %}confidence-high\n",
    "                            {% elif point.confidence_assessment.confidence_score > 0.4 %}confidence-medium\n",
    "                            {% else %}confidence-low{% endif %}\">\n",
    "                            {{ \"%.0f\"|format(point.confidence_assessment.confidence_score * 100) }}% Confidence\n",
    "                        </div>\n",
    "\n",
    "                        {% if point.confidence_assessment.key_strengths %}\n",
    "                        <h5>Key Strengths</h5>\n",
    "                        <ul>\n",
    "                            {% for strength in point.confidence_assessment.key_strengths %}\n",
    "                            <li>{{ strength }}</li>\n",
    "                            {% endfor %}\n",
    "                        </ul>\n",
    "                        {% endif %}\n",
    "\n",
    "                        {% if point.confidence_assessment.key_limitations %}\n",
    "                        <h5>Key Limitations</h5>\n",
    "                        <ul>\n",
    "                            {% for limitation in point.confidence_assessment.key_limitations %}\n",
    "                            <li>{{ limitation }}</li>\n",
    "                            {% endfor %}\n",
    "                        </ul>\n",
    "                        {% endif %}\n",
    "                    </div>\n",
    "                </div>\n",
    "\n",
    "                <details>\n",
    "                    <summary>Analysis Details</summary>\n",
    "                    <div class=\"rationale\">\n",
    "                        {% if point.analysis_rationale %}\n",
    "                        <pre><code>{{ point.analysis_rationale }}</code></pre>\n",
    "                        {% else %}\n",
    "                        <p>No detailed analysis rationale available.</p>\n",
    "                        {% endif %}\n",
    "                    </div>\n",
    "                </details>\n",
    "            </div>\n",
    "            {% endfor %}\n",
    "        </section>\n",
    "    </main>\n",
    "\n",
    "    <script>\n",
    "        const ctx = document.getElementById('beliefChart').getContext('2d');\n",
    "        new Chart(ctx, {\n",
    "            type: 'line',\n",
    "            data: {\n",
    "                labels: ['Prior'].concat({{ evidence_points|map(attribute='file')|list|tojson }}),\n",
    "                datasets: [{\n",
    "                    label: 'Belief Probability',\n",
    "                    data: [{{ prior_probability }}].concat({{ evidence_points|map(attribute='posterior')|list|tojson }}),\n",
    "                    borderColor: '#2b6cb0',\n",
    "                    backgroundColor: 'rgba(43, 108, 176, 0.1)',\n",
    "                    tension: 0.1\n",
    "                }]\n",
    "            },\n",
    "            options: {\n",
    "                responsive: true,\n",
    "                maintainAspectRatio: false,\n",
    "                scales: {\n",
    "                    y: {\n",
    "                        beginAtZero: true,\n",
    "                        max: 1,\n",
    "                        ticks: {\n",
    "                            callback: function(value) {\n",
    "                                return (value * 100) + '%';\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    x: {\n",
    "                        ticks: {\n",
    "                            maxRotation: 45,\n",
    "                            minRotation: 45\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                plugins: {\n",
    "                    tooltip: {\n",
    "                        callbacks: {\n",
    "                            label: function(context) {\n",
    "                                return (context.raw * 100).toFixed(1) + '%';\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ],
   "metadata": {
    "cellView": "form",
    "id": "_1LOD8Hfd4zU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "from pathlib import Path\n",
    "import math\n",
    "from jinja2 import Environment, FileSystemLoader, BaseLoader, Template\n",
    "\n",
    "class InFactRenderer:\n",
    "    def __init__(self, template_dir=None):\n",
    "        \"\"\"Initialize renderer with template directory or bundled template\"\"\"\n",
    "        if template_dir:\n",
    "            self.env = Environment(loader=FileSystemLoader(template_dir))\n",
    "        else:\n",
    "            self.env = Environment(loader=BaseLoader())\n",
    "            self.env.from_string(TEMPLATE)  # TEMPLATE would be the template we created above\n",
    "\n",
    "    def render_analysis(self, node, output_file=None):\n",
    "        \"\"\"Render complete analysis visualization\"\"\"\n",
    "        # template = self.env.get_template('infact.html')\n",
    "        template = Template(TEMPLATE)\n",
    "\n",
    "        # Prepare evidence points data\n",
    "        evidence_points = []\n",
    "        current_probability = 0\n",
    "\n",
    "\n",
    "        for point in node.data_points:\n",
    "            # Calculate probabilities\n",
    "            posterior = point['posterior']\n",
    "            likelihood_ratio = math.exp(point['l_plus'] - point['l_minus'])\n",
    "            prior_prob = math.exp(posterior - (point['l_plus'] - point['l_minus'])) / \\\n",
    "                        (1 + math.exp(posterior - (point['l_plus'] - point['l_minus'])))\n",
    "            posterior_prob = math.exp(posterior) / (1 + math.exp(posterior))\n",
    "\n",
    "            evidence_points.append({\n",
    "                'file': point['metadata'].get('filename', 'Unknown File'),\n",
    "                'confidence_assessment': point.get('confidence_assessment', {\n",
    "                    'confidence_score': 0,\n",
    "                    'explanation': 'No confidence assessment available',\n",
    "                    'key_strengths': [],\n",
    "                    'key_limitations': []\n",
    "                }),\n",
    "                'prior_prob': prior_prob,\n",
    "                'likelihood_ratio': likelihood_ratio,\n",
    "                'posterior': posterior_prob,\n",
    "                'analysis_rationale': point['analysis_rationale'] if 'analysis_rationale' in point else 'No analysis rationale available'\n",
    "            })\n",
    "\n",
    "            current_probability = posterior_prob\n",
    "\n",
    "        # Calculate prior probability from log odds\n",
    "        prior_probability = math.exp(node.prior_log_odds) / (1 + math.exp(node.prior_log_odds))\n",
    "\n",
    "        # Get confidence interval\n",
    "        ci_low, ci_high = node._calculate_uncertainty()\n",
    "\n",
    "        # Render template\n",
    "        html = template.render(\n",
    "            hypothesis=node.hypothesis,\n",
    "            prior_probability=prior_probability,\n",
    "            final_probability=current_probability,\n",
    "            ci_low=ci_low,\n",
    "            ci_high=ci_high,\n",
    "            evidence_points=evidence_points\n",
    "        )\n",
    "\n",
    "        # Save to file if requested\n",
    "        if output_file:\n",
    "            output_path = Path(output_file)\n",
    "            output_path.write_text(html)\n",
    "\n",
    "        return html"
   ],
   "metadata": {
    "cellView": "form",
    "id": "sol9WaTAd7X2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "# List files on directory, filtering out those that begin with .\n",
    "EVIDENCE_DIR = '/content/drive/MyDrive/InFact Prototype/evidence'\n",
    "evidence_files = list([str(p) for p in Path(EVIDENCE_DIR).glob('[!.]*.*')])\n",
    "evidence_files"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "brM52l7RkUuG",
    "outputId": "9b07bcf0-e5f7-481a-a29a-cf95b4fda4ad"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/InFact Prototype/evidence/climate-study.html',\n",
       " '/content/drive/MyDrive/InFact Prototype/evidence/emissions-analysis.html',\n",
       " '/content/drive/MyDrive/InFact Prototype/evidence/meta-analysis.html']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "from google.colab import userdata\n",
    "api_key = userdata.get('ANTHROPIC_API_KEY')"
   ],
   "metadata": {
    "cellView": "form",
    "id": "9PPlG-yalJl4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "import logging\n",
    "\n",
    "node = AnthropicInFactNode(\n",
    "    hypothesis=\"Human-generated GHG emissions significantly increase global temperatures\",\n",
    "    api_key=api_key,\n",
    "    log_level=logging.DEBUG  # For maximum detail\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "dO8_Pl2tlSXL",
    "outputId": "6e806575-091e-4ec1-886b-8bc4ac3bcca6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:__main__:Initializing AnthropicInFactNode with hypothesis: Human-generated GHG emissions significantly increase global temperatures\n",
      "INFO:__main__:Initialization complete\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "node.process_data(evidence_files[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "W989OO30mmOR",
    "outputId": "e04bb689-8941-465c-e63b-8e9e5cfcf2b0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:__main__:Processing data file: /content/drive/MyDrive/InFact Prototype/evidence/climate-study.html\n",
      "INFO:__main__:Parsing data file: /content/drive/MyDrive/InFact Prototype/evidence/climate-study.html\n",
      "DEBUG:__main__:Processing text file of type .html\n",
      "DEBUG:__main__:Prepared prompt: \n",
      "            Extract relevant data points for evaluating the hypothesis:\n",
      "            \"Human-generated GHG emissions significantly increase global temperatures\"\n",
      "            \n",
      "            Provide your response as a JSON code block, like this:\n",
      "            ```json\n",
      "            {\n",
      "                \"numerical_values\": [],\n",
      "                \"metadata\": {},\n",
      "                \"issues\": [],\n",
      "                \"confidence_assessment\": {\n",
      "                    \"confidence_score\": 0.75,\n",
      "                    \"explanation\": \"Detailed explanation of confidence level\",\n",
      "                    \"key_strengths\": [\n",
      "                        \"Strength 1\",\n",
      "                        \"Strength 2\"\n",
      "                    ],\n",
      "                    \"key_limitations\": [\n",
      "                        \"Limitation 1\",\n",
      "                        \"Limitation 2\"\n",
      "                    ]\n",
      "                }\n",
      "            }\n",
      "            ```\n",
      "\n",
      "            The confidence_assessment should:\n",
      "            1. Include a confidence_score between 0 and 1\n",
      "            2. Provide a detailed explanation of the confidence level\n",
      "            3. List key strengths of the evidence\n",
      "            4. List key limitations or potential issues\n",
      "\n",
      "            The overall JSON should include:\n",
      "            1. Extracted numerical values and their uncertainties\n",
      "            2. Relevant metadata (source quality, methodology, etc.)\n",
      "            3. Any potential issues or biases in the data\n",
      "            \n",
      "INFO:__main__:Sending request to Anthropic API\n",
      "DEBUG:__main__:Received API response: ```json\n",
      "{\n",
      "    \"numerical_values\": [\n",
      "        {\n",
      "            \"metric\": \"Arctic warming rate\",\n",
      "            \"value\": 2.7,\n",
      "            \"unit\": \"times global average\"\n",
      "        },\n",
      "        {\n",
      "            \"period\": \"1980-1989\",\n",
      "            \"anomaly\": 0.58,\n",
      "            \"uncertainty\": 0.12,\n",
      "            \"p_value\": 0.001\n",
      "        },\n",
      "        {\n",
      "            \"period\": \"2020-2023\", \n",
      "            \"anomaly\": 2.85,\n",
      "            \"uncertainty\": 0.08,\n",
      "            \"p_value\": 0.001\n",
      "        },\n",
      "        {\n",
      "            \"statistical_significance\": {\n",
      "                \"p_value\": 0.0001,\n",
      "                \"monte_carlo_iterations\": 10000\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"source\": \"Arctic Climate Research Institute\",\n",
      "        \"authors\": [\"Dr. Sarah Chen\", \"Dr. Marcus Peterson\"],\n",
      "        \"publication_date\": \"2024-01-15\",\n",
      "        \"methodology\": {\n",
      "            \"data_collection\": \"AVHRR satellite series\",\n",
      "            \"validation\": \"Ground-based weather stations\",\n",
      "            \"statistical_method\": \"Monte Carlo analysis\",\n",
      "            \"time_period\": \"1979-2023\",\n",
      "            \"peer_reviewed\": true\n",
      "        }\n",
      "    },\n",
      "    \"issues\": [\n",
      "        \"Regional focus (Arctic only) may not fully represent global patterns\",\n",
      "        \"Satellite data limited to post-1979 period\",\n",
      "        \"Ground station coverage in Arctic may be sparse\"\n",
      "    ],\n",
      "    \"confidence_assessment\": {\n",
      "        \"confidence_score\": 0.85,\n",
      "        \"explanation\": \"The study provides strong evidence for anthropogenic warming due to multiple factors: long-term data series (44 years), rigorous statistical analysis, cross-validation with ground stations, and extremely low p-values. The acceleration pattern aligns with increased GHG emissions, and the methodology includes uncertainty quantification.\",\n",
      "        \"key_strengths\": [\n",
      "            \"Long-term continuous satellite data (44 years)\",\n",
      "            \"Cross-validation with ground stations\",\n",
      "            \"Robust statistical analysis (Monte Carlo with 10,000 iterations)\",\n",
      "            \"Very low p-values (<0.001) across all decades\",\n",
      "            \"Clear acceleration trend matching GHG emission patterns\",\n",
      "            \"Uncertainty measurements included for all values\"\n",
      "        ],\n",
      "        \"key_limitations\": [\n",
      "            \"Focus on Arctic region only - may not represent global patterns\",\n",
      "            \"Relatively recent data (post-1979) compared to full climate record\",\n",
      "            \"Potential satellite calibration issues across different instruments\",\n",
      "            \"Does not directly measure GHG concentrations\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "```\n",
      "DEBUG:__main__:Successfully parsed JSON data: {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"metric\": \"Arctic warming rate\",\n",
      "      \"value\": 2.7,\n",
      "      \"unit\": \"times global average\"\n",
      "    },\n",
      "    {\n",
      "      \"period\": \"1980-1989\",\n",
      "      \"anomaly\": 0.58,\n",
      "      \"uncertainty\": 0.12,\n",
      "      \"p_value\": 0.001\n",
      "    },\n",
      "    {\n",
      "      \"period\": \"2020-2023\",\n",
      "      \"anomaly\": 2.85,\n",
      "      \"uncertainty\": 0.08,\n",
      "      \"p_value\": 0.001\n",
      "    },\n",
      "    {\n",
      "      \"statistical_significance\": {\n",
      "        \"p_value\": 0.0001,\n",
      "        \"monte_carlo_iterations\": 10000\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"source\": \"Arctic Climate Research Institute\",\n",
      "    \"authors\": [\n",
      "      \"Dr. Sarah Chen\",\n",
      "      \"Dr. Marcus Peterson\"\n",
      "    ],\n",
      "    \"publication_date\": \"2024-01-15\",\n",
      "    \"methodology\": {\n",
      "      \"data_collection\": \"AVHRR satellite series\",\n",
      "      \"validation\": \"Ground-based weather stations\",\n",
      "      \"statistical_method\": \"Monte Carlo analysis\",\n",
      "      \"time_period\": \"1979-2023\",\n",
      "      \"peer_reviewed\": true\n",
      "    }\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Regional focus (Arctic only) may not fully represent global patterns\",\n",
      "    \"Satellite data limited to post-1979 period\",\n",
      "    \"Ground station coverage in Arctic may be sparse\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The study provides strong evidence for anthropogenic warming due to multiple factors: long-term data series (44 years), rigorous statistical analysis, cross-validation with ground stations, and extremely low p-values. The acceleration pattern aligns with increased GHG emissions, and the methodology includes uncertainty quantification.\",\n",
      "    \"key_strengths\": [\n",
      "      \"Long-term continuous satellite data (44 years)\",\n",
      "      \"Cross-validation with ground stations\",\n",
      "      \"Robust statistical analysis (Monte Carlo with 10,000 iterations)\",\n",
      "      \"Very low p-values (<0.001) across all decades\",\n",
      "      \"Clear acceleration trend matching GHG emission patterns\",\n",
      "      \"Uncertainty measurements included for all values\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Focus on Arctic region only - may not represent global patterns\",\n",
      "      \"Relatively recent data (post-1979) compared to full climate record\",\n",
      "      \"Potential satellite calibration issues across different instruments\",\n",
      "      \"Does not directly measure GHG concentrations\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "DEBUG:__main__:Parsed data: {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"metric\": \"Arctic warming rate\",\n",
      "      \"value\": 2.7,\n",
      "      \"unit\": \"times global average\"\n",
      "    },\n",
      "    {\n",
      "      \"period\": \"1980-1989\",\n",
      "      \"anomaly\": 0.58,\n",
      "      \"uncertainty\": 0.12,\n",
      "      \"p_value\": 0.001\n",
      "    },\n",
      "    {\n",
      "      \"period\": \"2020-2023\",\n",
      "      \"anomaly\": 2.85,\n",
      "      \"uncertainty\": 0.08,\n",
      "      \"p_value\": 0.001\n",
      "    },\n",
      "    {\n",
      "      \"statistical_significance\": {\n",
      "        \"p_value\": 0.0001,\n",
      "        \"monte_carlo_iterations\": 10000\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"source\": \"Arctic Climate Research Institute\",\n",
      "    \"authors\": [\n",
      "      \"Dr. Sarah Chen\",\n",
      "      \"Dr. Marcus Peterson\"\n",
      "    ],\n",
      "    \"publication_date\": \"2024-01-15\",\n",
      "    \"methodology\": {\n",
      "      \"data_collection\": \"AVHRR satellite series\",\n",
      "      \"validation\": \"Ground-based weather stations\",\n",
      "      \"statistical_method\": \"Monte Carlo analysis\",\n",
      "      \"time_period\": \"1979-2023\",\n",
      "      \"peer_reviewed\": true\n",
      "    }\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Regional focus (Arctic only) may not fully represent global patterns\",\n",
      "    \"Satellite data limited to post-1979 period\",\n",
      "    \"Ground station coverage in Arctic may be sparse\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The study provides strong evidence for anthropogenic warming due to multiple factors: long-term data series (44 years), rigorous statistical analysis, cross-validation with ground stations, and extremely low p-values. The acceleration pattern aligns with increased GHG emissions, and the methodology includes uncertainty quantification.\",\n",
      "    \"key_strengths\": [\n",
      "      \"Long-term continuous satellite data (44 years)\",\n",
      "      \"Cross-validation with ground stations\",\n",
      "      \"Robust statistical analysis (Monte Carlo with 10,000 iterations)\",\n",
      "      \"Very low p-values (<0.001) across all decades\",\n",
      "      \"Clear acceleration trend matching GHG emission patterns\",\n",
      "      \"Uncertainty measurements included for all values\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Focus on Arctic region only - may not represent global patterns\",\n",
      "      \"Relatively recent data (post-1979) compared to full climate record\",\n",
      "      \"Potential satellite calibration issues across different instruments\",\n",
      "      \"Does not directly measure GHG concentrations\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "INFO:__main__:Checking for data redundancy\n",
      "DEBUG:__main__:No existing data points, not redundant\n",
      "INFO:__main__:Analyzing parsed data\n",
      "DEBUG:__main__:Analysis prompt: \n",
      "            Given this data:\n",
      "            {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"metric\": \"Arctic warming rate\",\n",
      "      \"value\": 2.7,\n",
      "      \"unit\": \"times global average\"\n",
      "    },\n",
      "    {\n",
      "      \"period\": \"1980-1989\",\n",
      "      \"anomaly\": 0.58,\n",
      "      \"uncertainty\": 0.12,\n",
      "      \"p_value\": 0.001\n",
      "    },\n",
      "    {\n",
      "      \"period\": \"2020-2023\",\n",
      "      \"anomaly\": 2.85,\n",
      "      \"uncertainty\": 0.08,\n",
      "      \"p_value\": 0.001\n",
      "    },\n",
      "    {\n",
      "      \"statistical_significance\": {\n",
      "        \"p_value\": 0.0001,\n",
      "        \"monte_carlo_iterations\": 10000\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"source\": \"Arctic Climate Research Institute\",\n",
      "    \"authors\": [\n",
      "      \"Dr. Sarah Chen\",\n",
      "      \"Dr. Marcus Peterson\"\n",
      "    ],\n",
      "    \"publication_date\": \"2024-01-15\",\n",
      "    \"methodology\": {\n",
      "      \"data_collection\": \"AVHRR satellite series\",\n",
      "      \"validation\": \"Ground-based weather stations\",\n",
      "      \"statistical_method\": \"Monte Carlo analysis\",\n",
      "      \"time_period\": \"1979-2023\",\n",
      "      \"peer_reviewed\": true\n",
      "    }\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Regional focus (Arctic only) may not fully represent global patterns\",\n",
      "    \"Satellite data limited to post-1979 period\",\n",
      "    \"Ground station coverage in Arctic may be sparse\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The study provides strong evidence for anthropogenic warming due to multiple factors: long-term data series (44 years), rigorous statistical analysis, cross-validation with ground stations, and extremely low p-values. The acceleration pattern aligns with increased GHG emissions, and the methodology includes uncertainty quantification.\",\n",
      "    \"key_strengths\": [\n",
      "      \"Long-term continuous satellite data (44 years)\",\n",
      "      \"Cross-validation with ground stations\",\n",
      "      \"Robust statistical analysis (Monte Carlo with 10,000 iterations)\",\n",
      "      \"Very low p-values (<0.001) across all decades\",\n",
      "      \"Clear acceleration trend matching GHG emission patterns\",\n",
      "      \"Uncertainty measurements included for all values\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Focus on Arctic region only - may not represent global patterns\",\n",
      "      \"Relatively recent data (post-1979) compared to full climate record\",\n",
      "      \"Potential satellite calibration issues across different instruments\",\n",
      "      \"Does not directly measure GHG concentrations\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "            \n",
      "            Generate Python code to calculate log likelihoods for the hypothesis:\n",
      "            \"Human-generated GHG emissions significantly increase global temperatures\"\n",
      "\n",
      "            This should be a single function named `calculate_log_likelihoods`.\n",
      "            It should take a single argument, a dict with the format given above,\n",
      "            and output only the tuple of log-likelihoods\n",
      "              l_plus = log P(data | hypothesis),\n",
      "              l_minus = log P(data | not hypothesis).\n",
      "            \n",
      "            The code should:\n",
      "            1. Calculate l_plus and l_minus (log likelihoods)\n",
      "            2. Handle uncertainties properly\n",
      "            3. Account for data quality and potential biases\n",
      "            4. Limit overconfidence by capping the absolute difference between l_plus and l_minus to 5.0.\n",
      "            4. Use the usual libraries such as numpy and scipy for calculations\n",
      "            5. Use print() to output intermediate results, as well as the final result before returning.\n",
      "            \n",
      "            Return only executable Python code with the function definition.\n",
      "            Do not include the function call itself.\n",
      "            \n",
      "DEBUG:__main__:API response: ```python\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key values\n",
      "    warming_rate = data[\"numerical_values\"][0][\"value\"]\n",
      "    early_anomaly = data[\"numerical_values\"][1][\"anomaly\"] \n",
      "    early_uncertainty = data[\"numerical_values\"][1][\"uncertainty\"]\n",
      "    recent_anomaly = data[\"numerical_values\"][2][\"anomaly\"]\n",
      "    recent_uncertainty = data[\"numerical_values\"][2][\"uncertainty\"]\n",
      "    confidence = data[\"confidence_assessment\"][\"confidence_score\"]\n",
      "    \n",
      "    # Calculate temperature change significance\n",
      "    z_score = (recent_anomaly - early_anomaly) / np.sqrt(early_uncertainty**2 + recent_uncertainty**2)\n",
      "    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
      "    print(f\"Temperature change z-score: {z_score:.2f}, p-value: {p_value:.4f}\")\n",
      "\n",
      "    # Calculate base likelihoods\n",
      "    # Higher likelihood under hypothesis if:\n",
      "    # - Large warming rate\n",
      "    # - Strong recent anomaly\n",
      "    # - Low p-value\n",
      "    # - High confidence score\n",
      "    l_plus_base = (\n",
      "        np.log(warming_rate) +\n",
      "        np.log(recent_anomaly + 1) + \n",
      "        -np.log(p_value + 1e-10) +\n",
      "        np.log(confidence)\n",
      "    )\n",
      "    \n",
      "    # Lower likelihood under null hypothesis\n",
      "    l_minus_base = (\n",
      "        -np.log(warming_rate) +\n",
      "        -np.log(recent_anomaly + 1) +\n",
      "        np.log(p_value + 1e-10) +\n",
      "        -np.log(confidence)\n",
      "    )\n",
      "    \n",
      "    print(f\"Base log likelihoods - l_plus: {l_plus_base:.2f}, l_minus: {l_minus_base:.2f}\")\n",
      "\n",
      "    # Adjust for data quality issues\n",
      "    quality_penalty = len(data[\"issues\"]) * 0.2\n",
      "    l_plus = l_plus_base - quality_penalty\n",
      "    l_minus = l_minus_base + quality_penalty\n",
      "    \n",
      "    print(f\"Quality-adjusted log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
      "\n",
      "    # Cap the absolute difference to avoid overconfidence\n",
      "    diff = l_plus - l_minus\n",
      "    if abs(diff) > 5.0:\n",
      "        adjustment = (abs(diff) - 5.0) / 2\n",
      "        if diff > 0:\n",
      "            l_plus -= adjustment\n",
      "            l_minus += adjustment\n",
      "        else:\n",
      "            l_plus += adjustment \n",
      "            l_minus -= adjustment\n",
      "            \n",
      "    print(f\"Final capped log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "```\n",
      "DEBUG:__main__:Extracted Python code: import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key values\n",
      "    warming_rate = data[\"numerical_values\"][0][\"value\"]\n",
      "    early_anomaly = data[\"numerical_values\"][1][\"anomaly\"] \n",
      "    early_uncertainty = data[\"numerical_values\"][1][\"uncertainty\"]\n",
      "    recent_anomaly = data[\"numerical_values\"][2][\"anomaly\"]\n",
      "    recent_uncertainty = data[\"numerical_values\"][2][\"uncertainty\"]\n",
      "    confidence = data[\"confidence_assessment\"][\"confidence_score\"]\n",
      "    \n",
      "    # Calculate temperature change significance\n",
      "    z_score = (recent_anomaly - early_anomaly) / np.sqrt(early_uncertainty**2 + recent_uncertainty**2)\n",
      "    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
      "    print(f\"Temperature change z-score: {z_score:.2f}, p-value: {p_value:.4f}\")\n",
      "\n",
      "    # Calculate base likelihoods\n",
      "    # Higher likelihood under hypothesis if:\n",
      "    # - Large warming rate\n",
      "    # - Strong recent anomaly\n",
      "    # - Low p-value\n",
      "    # - High confidence score\n",
      "    l_plus_base = (\n",
      "        np.log(warming_rate) +\n",
      "        np.log(recent_anomaly + 1) + \n",
      "        -np.log(p_value + 1e-10) +\n",
      "        np.log(confidence)\n",
      "    )\n",
      "    \n",
      "    # Lower likelihood under null hypothesis\n",
      "    l_minus_base = (\n",
      "        -np.log(warming_rate) +\n",
      "        -np.log(recent_anomaly + 1) +\n",
      "        np.log(p_value + 1e-10) +\n",
      "        -np.log(confidence)\n",
      "    )\n",
      "    \n",
      "    print(f\"Base log likelihoods - l_plus: {l_plus_base:.2f}, l_minus: {l_minus_base:.2f}\")\n",
      "\n",
      "    # Adjust for data quality issues\n",
      "    quality_penalty = len(data[\"issues\"]) * 0.2\n",
      "    l_plus = l_plus_base - quality_penalty\n",
      "    l_minus = l_minus_base + quality_penalty\n",
      "    \n",
      "    print(f\"Quality-adjusted log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
      "\n",
      "    # Cap the absolute difference to avoid overconfidence\n",
      "    diff = l_plus - l_minus\n",
      "    if abs(diff) > 5.0:\n",
      "        adjustment = (abs(diff) - 5.0) / 2\n",
      "        if diff > 0:\n",
      "            l_plus -= adjustment\n",
      "            l_minus += adjustment\n",
      "        else:\n",
      "            l_plus += adjustment \n",
      "            l_minus -= adjustment\n",
      "            \n",
      "    print(f\"Final capped log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "INFO:__main__:Code execution attempt 1/5\n",
      "DEBUG:__main__:Executing code:\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key values\n",
      "    warming_rate = data[\"numerical_values\"][0][\"value\"]\n",
      "    early_anomaly = data[\"numerical_values\"][1][\"anomaly\"] \n",
      "    early_uncertainty = data[\"numerical_values\"][1][\"uncertainty\"]\n",
      "    recent_anomaly = data[\"numerical_values\"][2][\"anomaly\"]\n",
      "    recent_uncertainty = data[\"numerical_values\"][2][\"uncertainty\"]\n",
      "    confidence = data[\"confidence_assessment\"][\"confidence_score\"]\n",
      "    \n",
      "    # Calculate temperature change significance\n",
      "    z_score = (recent_anomaly - early_anomaly) / np.sqrt(early_uncertainty**2 + recent_uncertainty**2)\n",
      "    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
      "    print(f\"Temperature change z-score: {z_score:.2f}, p-value: {p_value:.4f}\")\n",
      "\n",
      "    # Calculate base likelihoods\n",
      "    # Higher likelihood under hypothesis if:\n",
      "    # - Large warming rate\n",
      "    # - Strong recent anomaly\n",
      "    # - Low p-value\n",
      "    # - High confidence score\n",
      "    l_plus_base = (\n",
      "        np.log(warming_rate) +\n",
      "        np.log(recent_anomaly + 1) + \n",
      "        -np.log(p_value + 1e-10) +\n",
      "        np.log(confidence)\n",
      "    )\n",
      "    \n",
      "    # Lower likelihood under null hypothesis\n",
      "    l_minus_base = (\n",
      "        -np.log(warming_rate) +\n",
      "        -np.log(recent_anomaly + 1) +\n",
      "        np.log(p_value + 1e-10) +\n",
      "        -np.log(confidence)\n",
      "    )\n",
      "    \n",
      "    print(f\"Base log likelihoods - l_plus: {l_plus_base:.2f}, l_minus: {l_minus_base:.2f}\")\n",
      "\n",
      "    # Adjust for data quality issues\n",
      "    quality_penalty = len(data[\"issues\"]) * 0.2\n",
      "    l_plus = l_plus_base - quality_penalty\n",
      "    l_minus = l_minus_base + quality_penalty\n",
      "    \n",
      "    print(f\"Quality-adjusted log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
      "\n",
      "    # Cap the absolute difference to avoid overconfidence\n",
      "    diff = l_plus - l_minus\n",
      "    if abs(diff) > 5.0:\n",
      "        adjustment = (abs(diff) - 5.0) / 2\n",
      "        if diff > 0:\n",
      "            l_plus -= adjustment\n",
      "            l_minus += adjustment\n",
      "        else:\n",
      "            l_plus += adjustment \n",
      "            l_minus -= adjustment\n",
      "            \n",
      "    print(f\"Final capped log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "INFO:__main__:Code execution successful - l_plus: 2.5, l_minus: -2.5\n",
      "INFO:__main__:Analysis results - l_plus: 2.5, l_minus: -2.5\n",
      "INFO:__main__:Updated posterior from 0.0 to 5.0\n",
      "INFO:__main__:Extracting metadata from /content/drive/MyDrive/InFact Prototype/evidence/climate-study.html\n",
      "DEBUG:__main__:Extracted metadata: {\n",
      "  \"filename\": \"climate-study.html\",\n",
      "  \"file_type\": \".html\",\n",
      "  \"file_size\": 4678,\n",
      "  \"last_modified\": \"2025-01-04T18:25:29\",\n",
      "  \"source_path\": \"/content/drive/MyDrive/InFact Prototype/evidence/climate-study.html\",\n",
      "  \"title\": \"Temperature Anomalies in the Arctic Region: 1979-2023\",\n",
      "  \"meta_tags\": {\n",
      "    \"viewport\": \"width=device-width, initial-scale=1.0\",\n",
      "    \"author\": \"Dr. Sarah Chen, Arctic Climate Research Institute\",\n",
      "    \"description\": \"Analysis of satellite temperature data showing accelerated warming in the Arctic region\",\n",
      "    \"keywords\": \"Arctic, climate change, temperature anomalies, satellite data\",\n",
      "    \"publication-date\": \"2024-01-15\",\n",
      "    \"og:type\": \"article\",\n",
      "    \"og:site_name\": \"Arctic Climate Research Institute\"\n",
      "  },\n",
      "  \"has_article\": true,\n",
      "  \"has_main\": false,\n",
      "  \"num_headers\": 6,\n",
      "  \"has_tables\": true\n",
      "}\n",
      "INFO:__main__:Processing complete. Current probability: 99.33% (97.53%, 99.98%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Temperature change z-score: 15.74, p-value: 0.0000\n",
      "Base log likelihoods - l_plus: 25.20, l_minus: -25.20\n",
      "Quality-adjusted log likelihoods - l_plus: 24.60, l_minus: -24.60\n",
      "Final capped log likelihoods - l_plus: 2.50, l_minus: -2.50\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5.0, (0.9753088124811299, 0.9998365728296873))"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "node.process_data(evidence_files[1])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "JndrbtQo8iBn",
    "outputId": "dd5608de-57ab-4f04-e6d4-94d0ddc755d4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:__main__:Processing data file: /content/drive/MyDrive/InFact Prototype/evidence/emissions-analysis.html\n",
      "INFO:__main__:Parsing data file: /content/drive/MyDrive/InFact Prototype/evidence/emissions-analysis.html\n",
      "DEBUG:__main__:Processing text file of type .html\n",
      "DEBUG:__main__:Prepared prompt: \n",
      "            Extract relevant data points for evaluating the hypothesis:\n",
      "            \"Human-generated GHG emissions significantly increase global temperatures\"\n",
      "            \n",
      "            Provide your response as a JSON code block, like this:\n",
      "            ```json\n",
      "            {\n",
      "                \"numerical_values\": [],\n",
      "                \"metadata\": {},\n",
      "                \"issues\": [],\n",
      "                \"confidence_assessment\": {\n",
      "                    \"confidence_score\": 0.75,\n",
      "                    \"explanation\": \"Detailed explanation of confidence level\",\n",
      "                    \"key_strengths\": [\n",
      "                        \"Strength 1\",\n",
      "                        \"Strength 2\"\n",
      "                    ],\n",
      "                    \"key_limitations\": [\n",
      "                        \"Limitation 1\",\n",
      "                        \"Limitation 2\"\n",
      "                    ]\n",
      "                }\n",
      "            }\n",
      "            ```\n",
      "\n",
      "            The confidence_assessment should:\n",
      "            1. Include a confidence_score between 0 and 1\n",
      "            2. Provide a detailed explanation of the confidence level\n",
      "            3. List key strengths of the evidence\n",
      "            4. List key limitations or potential issues\n",
      "\n",
      "            The overall JSON should include:\n",
      "            1. Extracted numerical values and their uncertainties\n",
      "            2. Relevant metadata (source quality, methodology, etc.)\n",
      "            3. Any potential issues or biases in the data\n",
      "            \n",
      "INFO:__main__:Sending request to Anthropic API\n",
      "DEBUG:__main__:Received API response: ```json\n",
      "{\n",
      "    \"numerical_values\": [\n",
      "        {\n",
      "            \"type\": \"correlation\",\n",
      "            \"value\": 0.92,\n",
      "            \"p_value\": \"<0.001\",\n",
      "            \"description\": \"Statistical correlation between CO2 levels and temperature\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"r_squared\",\n",
      "            \"value\": 0.846,\n",
      "            \"p_value\": \"<0.001\",\n",
      "            \"description\": \"Variance in temperature explained by CO2 emissions\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"regression_coefficient\",\n",
      "            \"value\": 0.792,\n",
      "            \"std_error\": 0.034,\n",
      "            \"p_value\": \"<0.001\",\n",
      "            \"description\": \"CO2 emissions coefficient in multiple regression\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"emissions_increase\",\n",
      "            \"value\": 167,\n",
      "            \"unit\": \"percent\",\n",
      "            \"period\": \"since 1965\",\n",
      "            \"description\": \"Increase in annual global CO2 emissions\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"temperature_increase\",\n",
      "            \"value\": 0.98,\n",
      "            \"unit\": \"celsius\",\n",
      "            \"period\": \"since pre-industrial\",\n",
      "            \"description\": \"Global temperature anomaly increase\"\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"data_sources\": [\n",
      "            \"Global Carbon Project\",\n",
      "            \"NASA GISS temperature records\",\n",
      "            \"World Meteorological Organization\"\n",
      "        ],\n",
      "        \"analysis_tools\": \"R version 4.2.1 with tidyverse and stats packages\",\n",
      "        \"publication_date\": \"2023-12-10\",\n",
      "        \"methodology\": \"Multiple regression controlling for natural variability factors\"\n",
      "    },\n",
      "    \"issues\": [\n",
      "        \"Time period for temperature increase (pre-industrial) differs from emissions data (since 1965)\",\n",
      "        \"Natural variability factors show significant effects (solar activity p=0.003, volcanic activity p=0.018)\",\n",
      "        \"Emissions data grouped by decade may mask year-to-year variations\"\n",
      "    ],\n",
      "    \"confidence_assessment\": {\n",
      "        \"confidence_score\": 0.85,\n",
      "        \"explanation\": \"The data shows strong statistical evidence for human-caused warming through multiple analytical approaches, with high statistical significance and strong effect sizes. The analysis accounts for major natural variables and uses reputable data sources.\",\n",
      "        \"key_strengths\": [\n",
      "            \"High statistical significance (p<0.001) across multiple measures\",\n",
      "            \"Strong correlation coefficient (r=0.92) and R-squared value (0.846)\",\n",
      "            \"Analysis controls for natural variability factors\",\n",
      "            \"Data from authoritative sources\",\n",
      "            \"Multiple statistical approaches showing consistent results\"\n",
      "        ],\n",
      "        \"key_limitations\": [\n",
      "            \"Temporal mismatch between some measurements\",\n",
      "            \"Decade-level aggregation may obscure finer patterns\",\n",
      "            \"Natural factors still show significant effects\",\n",
      "            \"Limited data before 1965\",\n",
      "            \"Potential confounding variables not included in the analysis\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "```\n",
      "DEBUG:__main__:Successfully parsed JSON data: {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"type\": \"correlation\",\n",
      "      \"value\": 0.92,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Statistical correlation between CO2 levels and temperature\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"r_squared\",\n",
      "      \"value\": 0.846,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Variance in temperature explained by CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"regression_coefficient\",\n",
      "      \"value\": 0.792,\n",
      "      \"std_error\": 0.034,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"CO2 emissions coefficient in multiple regression\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"emissions_increase\",\n",
      "      \"value\": 167,\n",
      "      \"unit\": \"percent\",\n",
      "      \"period\": \"since 1965\",\n",
      "      \"description\": \"Increase in annual global CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"temperature_increase\",\n",
      "      \"value\": 0.98,\n",
      "      \"unit\": \"celsius\",\n",
      "      \"period\": \"since pre-industrial\",\n",
      "      \"description\": \"Global temperature anomaly increase\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"data_sources\": [\n",
      "      \"Global Carbon Project\",\n",
      "      \"NASA GISS temperature records\",\n",
      "      \"World Meteorological Organization\"\n",
      "    ],\n",
      "    \"analysis_tools\": \"R version 4.2.1 with tidyverse and stats packages\",\n",
      "    \"publication_date\": \"2023-12-10\",\n",
      "    \"methodology\": \"Multiple regression controlling for natural variability factors\"\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Time period for temperature increase (pre-industrial) differs from emissions data (since 1965)\",\n",
      "    \"Natural variability factors show significant effects (solar activity p=0.003, volcanic activity p=0.018)\",\n",
      "    \"Emissions data grouped by decade may mask year-to-year variations\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The data shows strong statistical evidence for human-caused warming through multiple analytical approaches, with high statistical significance and strong effect sizes. The analysis accounts for major natural variables and uses reputable data sources.\",\n",
      "    \"key_strengths\": [\n",
      "      \"High statistical significance (p<0.001) across multiple measures\",\n",
      "      \"Strong correlation coefficient (r=0.92) and R-squared value (0.846)\",\n",
      "      \"Analysis controls for natural variability factors\",\n",
      "      \"Data from authoritative sources\",\n",
      "      \"Multiple statistical approaches showing consistent results\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Temporal mismatch between some measurements\",\n",
      "      \"Decade-level aggregation may obscure finer patterns\",\n",
      "      \"Natural factors still show significant effects\",\n",
      "      \"Limited data before 1965\",\n",
      "      \"Potential confounding variables not included in the analysis\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "DEBUG:__main__:Parsed data: {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"type\": \"correlation\",\n",
      "      \"value\": 0.92,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Statistical correlation between CO2 levels and temperature\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"r_squared\",\n",
      "      \"value\": 0.846,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Variance in temperature explained by CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"regression_coefficient\",\n",
      "      \"value\": 0.792,\n",
      "      \"std_error\": 0.034,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"CO2 emissions coefficient in multiple regression\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"emissions_increase\",\n",
      "      \"value\": 167,\n",
      "      \"unit\": \"percent\",\n",
      "      \"period\": \"since 1965\",\n",
      "      \"description\": \"Increase in annual global CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"temperature_increase\",\n",
      "      \"value\": 0.98,\n",
      "      \"unit\": \"celsius\",\n",
      "      \"period\": \"since pre-industrial\",\n",
      "      \"description\": \"Global temperature anomaly increase\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"data_sources\": [\n",
      "      \"Global Carbon Project\",\n",
      "      \"NASA GISS temperature records\",\n",
      "      \"World Meteorological Organization\"\n",
      "    ],\n",
      "    \"analysis_tools\": \"R version 4.2.1 with tidyverse and stats packages\",\n",
      "    \"publication_date\": \"2023-12-10\",\n",
      "    \"methodology\": \"Multiple regression controlling for natural variability factors\"\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Time period for temperature increase (pre-industrial) differs from emissions data (since 1965)\",\n",
      "    \"Natural variability factors show significant effects (solar activity p=0.003, volcanic activity p=0.018)\",\n",
      "    \"Emissions data grouped by decade may mask year-to-year variations\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The data shows strong statistical evidence for human-caused warming through multiple analytical approaches, with high statistical significance and strong effect sizes. The analysis accounts for major natural variables and uses reputable data sources.\",\n",
      "    \"key_strengths\": [\n",
      "      \"High statistical significance (p<0.001) across multiple measures\",\n",
      "      \"Strong correlation coefficient (r=0.92) and R-squared value (0.846)\",\n",
      "      \"Analysis controls for natural variability factors\",\n",
      "      \"Data from authoritative sources\",\n",
      "      \"Multiple statistical approaches showing consistent results\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Temporal mismatch between some measurements\",\n",
      "      \"Decade-level aggregation may obscure finer patterns\",\n",
      "      \"Natural factors still show significant effects\",\n",
      "      \"Limited data before 1965\",\n",
      "      \"Potential confounding variables not included in the analysis\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "INFO:__main__:Checking for data redundancy\n",
      "DEBUG:__main__:Redundancy check prompt: \n",
      "            Compare the following new data:\n",
      "            {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"type\": \"correlation\",\n",
      "      \"value\": 0.92,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Statistical correlation between CO2 levels and temperature\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"r_squared\",\n",
      "      \"value\": 0.846,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Variance in temperature explained by CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"regression_coefficient\",\n",
      "      \"value\": 0.792,\n",
      "      \"std_error\": 0.034,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"CO2 emissions coefficient in multiple regression\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"emissions_increase\",\n",
      "      \"value\": 167,\n",
      "      \"unit\": \"percent\",\n",
      "      \"period\": \"since 1965\",\n",
      "      \"description\": \"Increase in annual global CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"temperature_increase\",\n",
      "      \"value\": 0.98,\n",
      "      \"unit\": \"celsius\",\n",
      "      \"period\": \"since pre-industrial\",\n",
      "      \"description\": \"Global temperature anomaly increase\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"data_sources\": [\n",
      "      \"Global Carbon Project\",\n",
      "      \"NASA GISS temperature records\",\n",
      "      \"World Meteorological Organization\"\n",
      "    ],\n",
      "    \"analysis_tools\": \"R version 4.2.1 with tidyverse and stats packages\",\n",
      "    \"publication_date\": \"2023-12-10\",\n",
      "    \"methodology\": \"Multiple regression controlling for natural variability factors\"\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Time period for temperature increase (pre-industrial) differs from emissions data (since 1965)\",\n",
      "    \"Natural variability factors show significant effects (solar activity p=0.003, volcanic activity p=0.018)\",\n",
      "    \"Emissions data grouped by decade may mask year-to-year variations\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The data shows strong statistical evidence for human-caused warming through multiple analytical approaches, with high statistical significance and strong effect sizes. The analysis accounts for major natural variables and uses reputable data sources.\",\n",
      "    \"key_strengths\": [\n",
      "      \"High statistical significance (p<0.001) across multiple measures\",\n",
      "      \"Strong correlation coefficient (r=0.92) and R-squared value (0.846)\",\n",
      "      \"Analysis controls for natural variability factors\",\n",
      "      \"Data from authoritative sources\",\n",
      "      \"Multiple statistical approaches showing consistent results\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Temporal mismatch between some measurements\",\n",
      "      \"Decade-level aggregation may obscure finer patterns\",\n",
      "      \"Natural factors still show significant effects\",\n",
      "      \"Limited data before 1965\",\n",
      "      \"Potential confounding variables not included in the analysis\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "            \n",
      "            With these existing data points:\n",
      "            [\n",
      "  {\n",
      "    \"numerical_values\": [\n",
      "      {\n",
      "        \"metric\": \"Arctic warming rate\",\n",
      "        \"value\": 2.7,\n",
      "        \"unit\": \"times global average\"\n",
      "      },\n",
      "      {\n",
      "        \"period\": \"1980-1989\",\n",
      "        \"anomaly\": 0.58,\n",
      "        \"uncertainty\": 0.12,\n",
      "        \"p_value\": 0.001\n",
      "      },\n",
      "      {\n",
      "        \"period\": \"2020-2023\",\n",
      "        \"anomaly\": 2.85,\n",
      "        \"uncertainty\": 0.08,\n",
      "        \"p_value\": 0.001\n",
      "      },\n",
      "      {\n",
      "        \"statistical_significance\": {\n",
      "          \"p_value\": 0.0001,\n",
      "          \"monte_carlo_iterations\": 10000\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "      \"source\": \"Arctic Climate Research Institute\",\n",
      "      \"authors\": [\n",
      "        \"Dr. Sarah Chen\",\n",
      "        \"Dr. Marcus Peterson\"\n",
      "      ],\n",
      "      \"publication_date\": \"2024-01-15\",\n",
      "      \"methodology\": {\n",
      "        \"data_collection\": \"AVHRR satellite series\",\n",
      "        \"validation\": \"Ground-based weather stations\",\n",
      "        \"statistical_method\": \"Monte Carlo analysis\",\n",
      "        \"time_period\": \"1979-2023\",\n",
      "        \"peer_reviewed\": true\n",
      "      }\n",
      "    },\n",
      "    \"issues\": [\n",
      "      \"Regional focus (Arctic only) may not fully represent global patterns\",\n",
      "      \"Satellite data limited to post-1979 period\",\n",
      "      \"Ground station coverage in Arctic may be sparse\"\n",
      "    ],\n",
      "    \"confidence_assessment\": {\n",
      "      \"confidence_score\": 0.85,\n",
      "      \"explanation\": \"The study provides strong evidence for anthropogenic warming due to multiple factors: long-term data series (44 years), rigorous statistical analysis, cross-validation with ground stations, and extremely low p-values. The acceleration pattern aligns with increased GHG emissions, and the methodology includes uncertainty quantification.\",\n",
      "      \"key_strengths\": [\n",
      "        \"Long-term continuous satellite data (44 years)\",\n",
      "        \"Cross-validation with ground stations\",\n",
      "        \"Robust statistical analysis (Monte Carlo with 10,000 iterations)\",\n",
      "        \"Very low p-values (<0.001) across all decades\",\n",
      "        \"Clear acceleration trend matching GHG emission patterns\",\n",
      "        \"Uncertainty measurements included for all values\"\n",
      "      ],\n",
      "      \"key_limitations\": [\n",
      "        \"Focus on Arctic region only - may not represent global patterns\",\n",
      "        \"Relatively recent data (post-1979) compared to full climate record\",\n",
      "        \"Potential satellite calibration issues across different instruments\",\n",
      "        \"Does not directly measure GHG concentrations\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "            \n",
      "            Is the new data redundant with any existing data points?\n",
      "            Consider:\n",
      "            1. Same source or study being cited\n",
      "            2. Same measurements within uncertainty\n",
      "            3. Derived results from already incorporated primary data\n",
      "            \n",
      "            Return \"true\" if redundant, \"false\" if novel information.\n",
      "            \n",
      "DEBUG:__main__:Redundancy check response: After comparing both datasets, I determine this is \"false\" - the data is not redundant.\n",
      "\n",
      "Key reasons:\n",
      "1. Different sources and scope:\n",
      "- New data comes from Global Carbon Project, NASA GISS, and WMO\n",
      "- Existing data comes from Arctic Climate Research Institute\n",
      "- Different authors and publication dates\n",
      "\n",
      "2. Different measurements and focus:\n",
      "- New data focuses on global CO2-temperature correlations and regression analysis\n",
      "- Existing data specifically examines Arctic warming rates and temperature anomalies\n",
      "- Different time periods (new data since 1965/pre-industrial, existing data 1979-2023)\n",
      "\n",
      "3. Complementary rather than redundant information:\n",
      "- New data provides global CO2-temperature relationships\n",
      "- Existing data focuses on regional Arctic amplification\n",
      "- Different statistical approaches (regression vs. Monte Carlo)\n",
      "- Different metrics (correlation coefficients vs. warming rates)\n",
      "\n",
      "While both datasets examine climate change, they provide distinct and complementary information rather than redundant measurements. The confidence assessments, while similar (both 0.85), are based on different criteria and strengths/limitations.\n",
      "\n",
      "Therefore, this represents novel information that adds to, rather than duplicates, the existing dataset.\n",
      "INFO:__main__:Redundancy check result: False\n",
      "INFO:__main__:Analyzing parsed data\n",
      "DEBUG:__main__:Analysis prompt: \n",
      "            Given this data:\n",
      "            {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"type\": \"correlation\",\n",
      "      \"value\": 0.92,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Statistical correlation between CO2 levels and temperature\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"r_squared\",\n",
      "      \"value\": 0.846,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"Variance in temperature explained by CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"regression_coefficient\",\n",
      "      \"value\": 0.792,\n",
      "      \"std_error\": 0.034,\n",
      "      \"p_value\": \"<0.001\",\n",
      "      \"description\": \"CO2 emissions coefficient in multiple regression\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"emissions_increase\",\n",
      "      \"value\": 167,\n",
      "      \"unit\": \"percent\",\n",
      "      \"period\": \"since 1965\",\n",
      "      \"description\": \"Increase in annual global CO2 emissions\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"temperature_increase\",\n",
      "      \"value\": 0.98,\n",
      "      \"unit\": \"celsius\",\n",
      "      \"period\": \"since pre-industrial\",\n",
      "      \"description\": \"Global temperature anomaly increase\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"data_sources\": [\n",
      "      \"Global Carbon Project\",\n",
      "      \"NASA GISS temperature records\",\n",
      "      \"World Meteorological Organization\"\n",
      "    ],\n",
      "    \"analysis_tools\": \"R version 4.2.1 with tidyverse and stats packages\",\n",
      "    \"publication_date\": \"2023-12-10\",\n",
      "    \"methodology\": \"Multiple regression controlling for natural variability factors\"\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Time period for temperature increase (pre-industrial) differs from emissions data (since 1965)\",\n",
      "    \"Natural variability factors show significant effects (solar activity p=0.003, volcanic activity p=0.018)\",\n",
      "    \"Emissions data grouped by decade may mask year-to-year variations\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The data shows strong statistical evidence for human-caused warming through multiple analytical approaches, with high statistical significance and strong effect sizes. The analysis accounts for major natural variables and uses reputable data sources.\",\n",
      "    \"key_strengths\": [\n",
      "      \"High statistical significance (p<0.001) across multiple measures\",\n",
      "      \"Strong correlation coefficient (r=0.92) and R-squared value (0.846)\",\n",
      "      \"Analysis controls for natural variability factors\",\n",
      "      \"Data from authoritative sources\",\n",
      "      \"Multiple statistical approaches showing consistent results\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Temporal mismatch between some measurements\",\n",
      "      \"Decade-level aggregation may obscure finer patterns\",\n",
      "      \"Natural factors still show significant effects\",\n",
      "      \"Limited data before 1965\",\n",
      "      \"Potential confounding variables not included in the analysis\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "            \n",
      "            Generate Python code to calculate log likelihoods for the hypothesis:\n",
      "            \"Human-generated GHG emissions significantly increase global temperatures\"\n",
      "\n",
      "            This should be a single function named `calculate_log_likelihoods`.\n",
      "            It should take a single argument, a dict with the format given above,\n",
      "            and output only the tuple of log-likelihoods\n",
      "              l_plus = log P(data | hypothesis),\n",
      "              l_minus = log P(data | not hypothesis).\n",
      "            \n",
      "            The code should:\n",
      "            1. Calculate l_plus and l_minus (log likelihoods)\n",
      "            2. Handle uncertainties properly\n",
      "            3. Account for data quality and potential biases\n",
      "            4. Limit overconfidence by capping the absolute difference between l_plus and l_minus to 5.0.\n",
      "            4. Use the usual libraries such as numpy and scipy for calculations\n",
      "            5. Use print() to output intermediate results, as well as the final result before returning.\n",
      "            \n",
      "            Return only executable Python code with the function definition.\n",
      "            Do not include the function call itself.\n",
      "            \n",
      "DEBUG:__main__:API response: ```python\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key statistical measures\n",
      "    correlation = data['numerical_values'][0]['value']\n",
      "    r_squared = data['numerical_values'][1]['value'] \n",
      "    regression_coef = data['numerical_values'][2]['value']\n",
      "    regression_se = data['numerical_values'][2]['std_error']\n",
      "    \n",
      "    # Calculate components for likelihood under H1 (hypothesis true)\n",
      "    \n",
      "    # 1. Correlation strength component\n",
      "    corr_component = stats.norm.logpdf(correlation, loc=0.8, scale=0.1)\n",
      "    print(f\"Correlation component: {corr_component:.3f}\")\n",
      "    \n",
      "    # 2. R-squared component \n",
      "    r2_component = stats.beta.logpdf(r_squared, a=20, b=4)\n",
      "    print(f\"R-squared component: {r2_component:.3f}\")\n",
      "    \n",
      "    # 3. Regression coefficient component\n",
      "    reg_z = regression_coef / regression_se\n",
      "    reg_component = stats.norm.logpdf(reg_z, loc=20, scale=5)\n",
      "    print(f\"Regression component: {reg_component:.3f}\")\n",
      "    \n",
      "    # 4. Data quality adjustment\n",
      "    confidence_score = data['confidence_assessment']['confidence_score']\n",
      "    quality_adjustment = np.log(confidence_score)\n",
      "    print(f\"Quality adjustment: {quality_adjustment:.3f}\")\n",
      "    \n",
      "    # Calculate l_plus (log likelihood under H1)\n",
      "    l_plus = corr_component + r2_component + reg_component + quality_adjustment\n",
      "    \n",
      "    # Calculate l_minus (log likelihood under H0)\n",
      "    # Assume weaker relationships under null hypothesis\n",
      "    null_corr = stats.norm.logpdf(correlation, loc=0.2, scale=0.2)\n",
      "    null_r2 = stats.beta.logpdf(r_squared, a=2, b=10)\n",
      "    null_reg = stats.norm.logpdf(reg_z, loc=0, scale=10)\n",
      "    \n",
      "    l_minus = null_corr + null_r2 + null_reg + quality_adjustment\n",
      "    \n",
      "    # Apply uncertainty penalties\n",
      "    n_limitations = len(data['confidence_assessment']['key_limitations'])\n",
      "    uncertainty_penalty = 0.2 * n_limitations\n",
      "    l_plus -= uncertainty_penalty\n",
      "    l_minus -= uncertainty_penalty\n",
      "    \n",
      "    print(f\"Uncertainty penalty: {uncertainty_penalty:.3f}\")\n",
      "    \n",
      "    # Cap the absolute difference between l_plus and l_minus\n",
      "    diff = l_plus - l_minus\n",
      "    if abs(diff) > 5.0:\n",
      "        adjustment = abs(diff) - 5.0\n",
      "        if diff > 0:\n",
      "            l_plus -= adjustment/2\n",
      "            l_minus += adjustment/2\n",
      "        else:\n",
      "            l_plus += adjustment/2\n",
      "            l_minus -= adjustment/2\n",
      "            \n",
      "    print(f\"\\nFinal log likelihoods:\")\n",
      "    print(f\"l_plus: {l_plus:.3f}\")\n",
      "    print(f\"l_minus: {l_minus:.3f}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "```\n",
      "DEBUG:__main__:Extracted Python code: import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key statistical measures\n",
      "    correlation = data['numerical_values'][0]['value']\n",
      "    r_squared = data['numerical_values'][1]['value'] \n",
      "    regression_coef = data['numerical_values'][2]['value']\n",
      "    regression_se = data['numerical_values'][2]['std_error']\n",
      "    \n",
      "    # Calculate components for likelihood under H1 (hypothesis true)\n",
      "    \n",
      "    # 1. Correlation strength component\n",
      "    corr_component = stats.norm.logpdf(correlation, loc=0.8, scale=0.1)\n",
      "    print(f\"Correlation component: {corr_component:.3f}\")\n",
      "    \n",
      "    # 2. R-squared component \n",
      "    r2_component = stats.beta.logpdf(r_squared, a=20, b=4)\n",
      "    print(f\"R-squared component: {r2_component:.3f}\")\n",
      "    \n",
      "    # 3. Regression coefficient component\n",
      "    reg_z = regression_coef / regression_se\n",
      "    reg_component = stats.norm.logpdf(reg_z, loc=20, scale=5)\n",
      "    print(f\"Regression component: {reg_component:.3f}\")\n",
      "    \n",
      "    # 4. Data quality adjustment\n",
      "    confidence_score = data['confidence_assessment']['confidence_score']\n",
      "    quality_adjustment = np.log(confidence_score)\n",
      "    print(f\"Quality adjustment: {quality_adjustment:.3f}\")\n",
      "    \n",
      "    # Calculate l_plus (log likelihood under H1)\n",
      "    l_plus = corr_component + r2_component + reg_component + quality_adjustment\n",
      "    \n",
      "    # Calculate l_minus (log likelihood under H0)\n",
      "    # Assume weaker relationships under null hypothesis\n",
      "    null_corr = stats.norm.logpdf(correlation, loc=0.2, scale=0.2)\n",
      "    null_r2 = stats.beta.logpdf(r_squared, a=2, b=10)\n",
      "    null_reg = stats.norm.logpdf(reg_z, loc=0, scale=10)\n",
      "    \n",
      "    l_minus = null_corr + null_r2 + null_reg + quality_adjustment\n",
      "    \n",
      "    # Apply uncertainty penalties\n",
      "    n_limitations = len(data['confidence_assessment']['key_limitations'])\n",
      "    uncertainty_penalty = 0.2 * n_limitations\n",
      "    l_plus -= uncertainty_penalty\n",
      "    l_minus -= uncertainty_penalty\n",
      "    \n",
      "    print(f\"Uncertainty penalty: {uncertainty_penalty:.3f}\")\n",
      "    \n",
      "    # Cap the absolute difference between l_plus and l_minus\n",
      "    diff = l_plus - l_minus\n",
      "    if abs(diff) > 5.0:\n",
      "        adjustment = abs(diff) - 5.0\n",
      "        if diff > 0:\n",
      "            l_plus -= adjustment/2\n",
      "            l_minus += adjustment/2\n",
      "        else:\n",
      "            l_plus += adjustment/2\n",
      "            l_minus -= adjustment/2\n",
      "            \n",
      "    print(f\"\\nFinal log likelihoods:\")\n",
      "    print(f\"l_plus: {l_plus:.3f}\")\n",
      "    print(f\"l_minus: {l_minus:.3f}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "INFO:__main__:Code execution attempt 1/5\n",
      "DEBUG:__main__:Executing code:\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key statistical measures\n",
      "    correlation = data['numerical_values'][0]['value']\n",
      "    r_squared = data['numerical_values'][1]['value'] \n",
      "    regression_coef = data['numerical_values'][2]['value']\n",
      "    regression_se = data['numerical_values'][2]['std_error']\n",
      "    \n",
      "    # Calculate components for likelihood under H1 (hypothesis true)\n",
      "    \n",
      "    # 1. Correlation strength component\n",
      "    corr_component = stats.norm.logpdf(correlation, loc=0.8, scale=0.1)\n",
      "    print(f\"Correlation component: {corr_component:.3f}\")\n",
      "    \n",
      "    # 2. R-squared component \n",
      "    r2_component = stats.beta.logpdf(r_squared, a=20, b=4)\n",
      "    print(f\"R-squared component: {r2_component:.3f}\")\n",
      "    \n",
      "    # 3. Regression coefficient component\n",
      "    reg_z = regression_coef / regression_se\n",
      "    reg_component = stats.norm.logpdf(reg_z, loc=20, scale=5)\n",
      "    print(f\"Regression component: {reg_component:.3f}\")\n",
      "    \n",
      "    # 4. Data quality adjustment\n",
      "    confidence_score = data['confidence_assessment']['confidence_score']\n",
      "    quality_adjustment = np.log(confidence_score)\n",
      "    print(f\"Quality adjustment: {quality_adjustment:.3f}\")\n",
      "    \n",
      "    # Calculate l_plus (log likelihood under H1)\n",
      "    l_plus = corr_component + r2_component + reg_component + quality_adjustment\n",
      "    \n",
      "    # Calculate l_minus (log likelihood under H0)\n",
      "    # Assume weaker relationships under null hypothesis\n",
      "    null_corr = stats.norm.logpdf(correlation, loc=0.2, scale=0.2)\n",
      "    null_r2 = stats.beta.logpdf(r_squared, a=2, b=10)\n",
      "    null_reg = stats.norm.logpdf(reg_z, loc=0, scale=10)\n",
      "    \n",
      "    l_minus = null_corr + null_r2 + null_reg + quality_adjustment\n",
      "    \n",
      "    # Apply uncertainty penalties\n",
      "    n_limitations = len(data['confidence_assessment']['key_limitations'])\n",
      "    uncertainty_penalty = 0.2 * n_limitations\n",
      "    l_plus -= uncertainty_penalty\n",
      "    l_minus -= uncertainty_penalty\n",
      "    \n",
      "    print(f\"Uncertainty penalty: {uncertainty_penalty:.3f}\")\n",
      "    \n",
      "    # Cap the absolute difference between l_plus and l_minus\n",
      "    diff = l_plus - l_minus\n",
      "    if abs(diff) > 5.0:\n",
      "        adjustment = abs(diff) - 5.0\n",
      "        if diff > 0:\n",
      "            l_plus -= adjustment/2\n",
      "            l_minus += adjustment/2\n",
      "        else:\n",
      "            l_plus += adjustment/2\n",
      "            l_minus -= adjustment/2\n",
      "            \n",
      "    print(f\"\\nFinal log likelihoods:\")\n",
      "    print(f\"l_plus: {l_plus:.3f}\")\n",
      "    print(f\"l_minus: {l_minus:.3f}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "INFO:__main__:Code execution successful - l_plus: -10.87486701362687, l_minus: -15.87486701362687\n",
      "INFO:__main__:Analysis results - l_plus: -10.87486701362687, l_minus: -15.87486701362687\n",
      "INFO:__main__:Updated posterior from 5.0 to 10.0\n",
      "INFO:__main__:Extracting metadata from /content/drive/MyDrive/InFact Prototype/evidence/emissions-analysis.html\n",
      "DEBUG:__main__:Extracted metadata: {\n",
      "  \"filename\": \"emissions-analysis.html\",\n",
      "  \"file_type\": \".html\",\n",
      "  \"file_size\": 5278,\n",
      "  \"last_modified\": \"2025-01-04T18:25:35\",\n",
      "  \"source_path\": \"/content/drive/MyDrive/InFact Prototype/evidence/emissions-analysis.html\",\n",
      "  \"title\": \"Global CO2 Emissions and Temperature Records: An Interactive Analysis\",\n",
      "  \"meta_tags\": {\n",
      "    \"viewport\": \"width=device-width, initial-scale=1.0\",\n",
      "    \"author\": \"Climate Data Initiative\",\n",
      "    \"description\": \"Interactive visualization of the relationship between CO2 emissions and global temperature rise\",\n",
      "    \"keywords\": \"CO2 emissions, global warming, data visualization, climate change\",\n",
      "    \"publication-date\": \"2023-12-10\",\n",
      "    \"og:type\": \"article\",\n",
      "    \"og:site_name\": \"Climate Data Initiative\"\n",
      "  },\n",
      "  \"has_article\": false,\n",
      "  \"has_main\": true,\n",
      "  \"num_headers\": 6,\n",
      "  \"has_tables\": true\n",
      "}\n",
      "INFO:__main__:Processing complete. Current probability: 100.00% (99.97%, 100.00%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correlation component: 0.664\n",
      "R-squared component: 1.685\n",
      "Regression component: -2.745\n",
      "Quality adjustment: -0.163\n",
      "Uncertainty penalty: 1.000\n",
      "\n",
      "Final log likelihoods:\n",
      "l_plus: -10.875\n",
      "l_minus: -15.875\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10.0, (0.9996816261099933, 1.0))"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "node.process_data(evidence_files[2])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "QHpM33cgDfKW",
    "outputId": "0a1369b1-36dd-4b88-d3f8-bd75cf3e8561"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:__main__:Processing data file: /content/drive/MyDrive/InFact Prototype/evidence/meta-analysis.html\n",
      "INFO:__main__:Parsing data file: /content/drive/MyDrive/InFact Prototype/evidence/meta-analysis.html\n",
      "DEBUG:__main__:Processing text file of type .html\n",
      "DEBUG:__main__:Prepared prompt: \n",
      "            Extract relevant data points for evaluating the hypothesis:\n",
      "            \"Human-generated GHG emissions significantly increase global temperatures\"\n",
      "            \n",
      "            Provide your response as a JSON code block, like this:\n",
      "            ```json\n",
      "            {\n",
      "                \"numerical_values\": [],\n",
      "                \"metadata\": {},\n",
      "                \"issues\": [],\n",
      "                \"confidence_assessment\": {\n",
      "                    \"confidence_score\": 0.75,\n",
      "                    \"explanation\": \"Detailed explanation of confidence level\",\n",
      "                    \"key_strengths\": [\n",
      "                        \"Strength 1\",\n",
      "                        \"Strength 2\"\n",
      "                    ],\n",
      "                    \"key_limitations\": [\n",
      "                        \"Limitation 1\",\n",
      "                        \"Limitation 2\"\n",
      "                    ]\n",
      "                }\n",
      "            }\n",
      "            ```\n",
      "\n",
      "            The confidence_assessment should:\n",
      "            1. Include a confidence_score between 0 and 1\n",
      "            2. Provide a detailed explanation of the confidence level\n",
      "            3. List key strengths of the evidence\n",
      "            4. List key limitations or potential issues\n",
      "\n",
      "            The overall JSON should include:\n",
      "            1. Extracted numerical values and their uncertainties\n",
      "            2. Relevant metadata (source quality, methodology, etc.)\n",
      "            3. Any potential issues or biases in the data\n",
      "            \n",
      "INFO:__main__:Sending request to Anthropic API\n",
      "DEBUG:__main__:Received API response: ```json\n",
      "{\n",
      "    \"numerical_values\": [\n",
      "        {\n",
      "            \"metric\": \"equilibrium_climate_sensitivity_best_estimate\",\n",
      "            \"value\": 3.4,\n",
      "            \"units\": \"C per CO2 doubling\",\n",
      "            \"uncertainty_range\": [2.8, 4.0]\n",
      "        },\n",
      "        {\n",
      "            \"metric\": \"projected_warming_2100_high_emissions\",\n",
      "            \"value\": 4.7,\n",
      "            \"units\": \"C\",\n",
      "            \"uncertainty_range\": [4.0, 5.4],\n",
      "            \"co2_level\": 850\n",
      "        },\n",
      "        {\n",
      "            \"metric\": \"projected_warming_2100_low_emissions\", \n",
      "            \"value\": 1.8,\n",
      "            \"units\": \"C\",\n",
      "            \"uncertainty_range\": [1.5, 2.1],\n",
      "            \"co2_level\": 450\n",
      "        },\n",
      "        {\n",
      "            \"metric\": \"uncertainty_reduction\",\n",
      "            \"value\": 1.3,\n",
      "            \"units\": \"C\",\n",
      "            \"description\": \"Reduction in ECS uncertainty range from 2000-2004 (2.5C) to 2020-2023 (1.2C)\"\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"study_count\": 147,\n",
      "        \"time_period\": \"2000-2023\",\n",
      "        \"study_types\": [\"Instrumental Record\", \"Paleoclimate\", \"Climate Models\", \"Process Studies\"],\n",
      "        \"quality_scores\": {\n",
      "            \"instrumental\": 8.4,\n",
      "            \"paleoclimate\": 7.9,\n",
      "            \"climate_models\": 8.7,\n",
      "            \"process_studies\": 8.2\n",
      "        },\n",
      "        \"methodology\": \"Bayesian hierarchical model meta-analysis\",\n",
      "        \"peer_reviewed\": true\n",
      "    },\n",
      "    \"issues\": [\n",
      "        \"Potential publication bias in study selection\",\n",
      "        \"Varying methodologies across studies may affect comparability\",\n",
      "        \"Historical data limitations in paleoclimate studies\",\n",
      "        \"Model-based projections inherit uncertainties from underlying assumptions\"\n",
      "    ],\n",
      "    \"confidence_assessment\": {\n",
      "        \"confidence_score\": 0.85,\n",
      "        \"explanation\": \"The meta-analysis provides strong evidence for human-induced warming through GHG emissions, supported by multiple independent lines of evidence showing consistent results. The convergence of estimates across different methodologies and narrowing uncertainty ranges over time strengthen the conclusions.\",\n",
      "        \"key_strengths\": [\n",
      "            \"Large sample size (147 peer-reviewed studies)\",\n",
      "            \"Multiple independent methodologies showing consistent results\",\n",
      "            \"Rigorous statistical analysis using Bayesian hierarchical modeling\",\n",
      "            \"Significant narrowing of uncertainty ranges over time\",\n",
      "            \"High quality scores across study types (7.9-8.7/10)\",\n",
      "            \"Clear quantification of uncertainties and confidence intervals\"\n",
      "        ],\n",
      "        \"key_limitations\": [\n",
      "            \"Remaining uncertainty in exact sensitivity values\",\n",
      "            \"Complex interactions in climate system may not be fully captured\",\n",
      "            \"Future projections depend on emission scenario assumptions\",\n",
      "            \"Potential systematic biases in historical data reconstruction\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "```\n",
      "DEBUG:__main__:Successfully parsed JSON data: {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"metric\": \"equilibrium_climate_sensitivity_best_estimate\",\n",
      "      \"value\": 3.4,\n",
      "      \"units\": \"\\u00b0C per CO2 doubling\",\n",
      "      \"uncertainty_range\": [\n",
      "        2.8,\n",
      "        4.0\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_high_emissions\",\n",
      "      \"value\": 4.7,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        4.0,\n",
      "        5.4\n",
      "      ],\n",
      "      \"co2_level\": 850\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_low_emissions\",\n",
      "      \"value\": 1.8,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        1.5,\n",
      "        2.1\n",
      "      ],\n",
      "      \"co2_level\": 450\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"uncertainty_reduction\",\n",
      "      \"value\": 1.3,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"description\": \"Reduction in ECS uncertainty range from 2000-2004 (2.5\\u00b0C) to 2020-2023 (1.2\\u00b0C)\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"study_count\": 147,\n",
      "    \"time_period\": \"2000-2023\",\n",
      "    \"study_types\": [\n",
      "      \"Instrumental Record\",\n",
      "      \"Paleoclimate\",\n",
      "      \"Climate Models\",\n",
      "      \"Process Studies\"\n",
      "    ],\n",
      "    \"quality_scores\": {\n",
      "      \"instrumental\": 8.4,\n",
      "      \"paleoclimate\": 7.9,\n",
      "      \"climate_models\": 8.7,\n",
      "      \"process_studies\": 8.2\n",
      "    },\n",
      "    \"methodology\": \"Bayesian hierarchical model meta-analysis\",\n",
      "    \"peer_reviewed\": true\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Potential publication bias in study selection\",\n",
      "    \"Varying methodologies across studies may affect comparability\",\n",
      "    \"Historical data limitations in paleoclimate studies\",\n",
      "    \"Model-based projections inherit uncertainties from underlying assumptions\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The meta-analysis provides strong evidence for human-induced warming through GHG emissions, supported by multiple independent lines of evidence showing consistent results. The convergence of estimates across different methodologies and narrowing uncertainty ranges over time strengthen the conclusions.\",\n",
      "    \"key_strengths\": [\n",
      "      \"Large sample size (147 peer-reviewed studies)\",\n",
      "      \"Multiple independent methodologies showing consistent results\",\n",
      "      \"Rigorous statistical analysis using Bayesian hierarchical modeling\",\n",
      "      \"Significant narrowing of uncertainty ranges over time\",\n",
      "      \"High quality scores across study types (7.9-8.7/10)\",\n",
      "      \"Clear quantification of uncertainties and confidence intervals\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Remaining uncertainty in exact sensitivity values\",\n",
      "      \"Complex interactions in climate system may not be fully captured\",\n",
      "      \"Future projections depend on emission scenario assumptions\",\n",
      "      \"Potential systematic biases in historical data reconstruction\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "DEBUG:__main__:Parsed data: {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"metric\": \"equilibrium_climate_sensitivity_best_estimate\",\n",
      "      \"value\": 3.4,\n",
      "      \"units\": \"\\u00b0C per CO2 doubling\",\n",
      "      \"uncertainty_range\": [\n",
      "        2.8,\n",
      "        4.0\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_high_emissions\",\n",
      "      \"value\": 4.7,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        4.0,\n",
      "        5.4\n",
      "      ],\n",
      "      \"co2_level\": 850\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_low_emissions\",\n",
      "      \"value\": 1.8,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        1.5,\n",
      "        2.1\n",
      "      ],\n",
      "      \"co2_level\": 450\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"uncertainty_reduction\",\n",
      "      \"value\": 1.3,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"description\": \"Reduction in ECS uncertainty range from 2000-2004 (2.5\\u00b0C) to 2020-2023 (1.2\\u00b0C)\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"study_count\": 147,\n",
      "    \"time_period\": \"2000-2023\",\n",
      "    \"study_types\": [\n",
      "      \"Instrumental Record\",\n",
      "      \"Paleoclimate\",\n",
      "      \"Climate Models\",\n",
      "      \"Process Studies\"\n",
      "    ],\n",
      "    \"quality_scores\": {\n",
      "      \"instrumental\": 8.4,\n",
      "      \"paleoclimate\": 7.9,\n",
      "      \"climate_models\": 8.7,\n",
      "      \"process_studies\": 8.2\n",
      "    },\n",
      "    \"methodology\": \"Bayesian hierarchical model meta-analysis\",\n",
      "    \"peer_reviewed\": true\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Potential publication bias in study selection\",\n",
      "    \"Varying methodologies across studies may affect comparability\",\n",
      "    \"Historical data limitations in paleoclimate studies\",\n",
      "    \"Model-based projections inherit uncertainties from underlying assumptions\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The meta-analysis provides strong evidence for human-induced warming through GHG emissions, supported by multiple independent lines of evidence showing consistent results. The convergence of estimates across different methodologies and narrowing uncertainty ranges over time strengthen the conclusions.\",\n",
      "    \"key_strengths\": [\n",
      "      \"Large sample size (147 peer-reviewed studies)\",\n",
      "      \"Multiple independent methodologies showing consistent results\",\n",
      "      \"Rigorous statistical analysis using Bayesian hierarchical modeling\",\n",
      "      \"Significant narrowing of uncertainty ranges over time\",\n",
      "      \"High quality scores across study types (7.9-8.7/10)\",\n",
      "      \"Clear quantification of uncertainties and confidence intervals\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Remaining uncertainty in exact sensitivity values\",\n",
      "      \"Complex interactions in climate system may not be fully captured\",\n",
      "      \"Future projections depend on emission scenario assumptions\",\n",
      "      \"Potential systematic biases in historical data reconstruction\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "INFO:__main__:Checking for data redundancy\n",
      "DEBUG:__main__:Redundancy check prompt: \n",
      "            Compare the following new data:\n",
      "            {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"metric\": \"equilibrium_climate_sensitivity_best_estimate\",\n",
      "      \"value\": 3.4,\n",
      "      \"units\": \"\\u00b0C per CO2 doubling\",\n",
      "      \"uncertainty_range\": [\n",
      "        2.8,\n",
      "        4.0\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_high_emissions\",\n",
      "      \"value\": 4.7,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        4.0,\n",
      "        5.4\n",
      "      ],\n",
      "      \"co2_level\": 850\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_low_emissions\",\n",
      "      \"value\": 1.8,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        1.5,\n",
      "        2.1\n",
      "      ],\n",
      "      \"co2_level\": 450\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"uncertainty_reduction\",\n",
      "      \"value\": 1.3,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"description\": \"Reduction in ECS uncertainty range from 2000-2004 (2.5\\u00b0C) to 2020-2023 (1.2\\u00b0C)\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"study_count\": 147,\n",
      "    \"time_period\": \"2000-2023\",\n",
      "    \"study_types\": [\n",
      "      \"Instrumental Record\",\n",
      "      \"Paleoclimate\",\n",
      "      \"Climate Models\",\n",
      "      \"Process Studies\"\n",
      "    ],\n",
      "    \"quality_scores\": {\n",
      "      \"instrumental\": 8.4,\n",
      "      \"paleoclimate\": 7.9,\n",
      "      \"climate_models\": 8.7,\n",
      "      \"process_studies\": 8.2\n",
      "    },\n",
      "    \"methodology\": \"Bayesian hierarchical model meta-analysis\",\n",
      "    \"peer_reviewed\": true\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Potential publication bias in study selection\",\n",
      "    \"Varying methodologies across studies may affect comparability\",\n",
      "    \"Historical data limitations in paleoclimate studies\",\n",
      "    \"Model-based projections inherit uncertainties from underlying assumptions\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The meta-analysis provides strong evidence for human-induced warming through GHG emissions, supported by multiple independent lines of evidence showing consistent results. The convergence of estimates across different methodologies and narrowing uncertainty ranges over time strengthen the conclusions.\",\n",
      "    \"key_strengths\": [\n",
      "      \"Large sample size (147 peer-reviewed studies)\",\n",
      "      \"Multiple independent methodologies showing consistent results\",\n",
      "      \"Rigorous statistical analysis using Bayesian hierarchical modeling\",\n",
      "      \"Significant narrowing of uncertainty ranges over time\",\n",
      "      \"High quality scores across study types (7.9-8.7/10)\",\n",
      "      \"Clear quantification of uncertainties and confidence intervals\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Remaining uncertainty in exact sensitivity values\",\n",
      "      \"Complex interactions in climate system may not be fully captured\",\n",
      "      \"Future projections depend on emission scenario assumptions\",\n",
      "      \"Potential systematic biases in historical data reconstruction\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "            \n",
      "            With these existing data points:\n",
      "            [\n",
      "  {\n",
      "    \"numerical_values\": [\n",
      "      {\n",
      "        \"metric\": \"Arctic warming rate\",\n",
      "        \"value\": 2.7,\n",
      "        \"unit\": \"times global average\"\n",
      "      },\n",
      "      {\n",
      "        \"period\": \"1980-1989\",\n",
      "        \"anomaly\": 0.58,\n",
      "        \"uncertainty\": 0.12,\n",
      "        \"p_value\": 0.001\n",
      "      },\n",
      "      {\n",
      "        \"period\": \"2020-2023\",\n",
      "        \"anomaly\": 2.85,\n",
      "        \"uncertainty\": 0.08,\n",
      "        \"p_value\": 0.001\n",
      "      },\n",
      "      {\n",
      "        \"statistical_significance\": {\n",
      "          \"p_value\": 0.0001,\n",
      "          \"monte_carlo_iterations\": 10000\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "      \"source\": \"Arctic Climate Research Institute\",\n",
      "      \"authors\": [\n",
      "        \"Dr. Sarah Chen\",\n",
      "        \"Dr. Marcus Peterson\"\n",
      "      ],\n",
      "      \"publication_date\": \"2024-01-15\",\n",
      "      \"methodology\": {\n",
      "        \"data_collection\": \"AVHRR satellite series\",\n",
      "        \"validation\": \"Ground-based weather stations\",\n",
      "        \"statistical_method\": \"Monte Carlo analysis\",\n",
      "        \"time_period\": \"1979-2023\",\n",
      "        \"peer_reviewed\": true\n",
      "      }\n",
      "    },\n",
      "    \"issues\": [\n",
      "      \"Regional focus (Arctic only) may not fully represent global patterns\",\n",
      "      \"Satellite data limited to post-1979 period\",\n",
      "      \"Ground station coverage in Arctic may be sparse\"\n",
      "    ],\n",
      "    \"confidence_assessment\": {\n",
      "      \"confidence_score\": 0.85,\n",
      "      \"explanation\": \"The study provides strong evidence for anthropogenic warming due to multiple factors: long-term data series (44 years), rigorous statistical analysis, cross-validation with ground stations, and extremely low p-values. The acceleration pattern aligns with increased GHG emissions, and the methodology includes uncertainty quantification.\",\n",
      "      \"key_strengths\": [\n",
      "        \"Long-term continuous satellite data (44 years)\",\n",
      "        \"Cross-validation with ground stations\",\n",
      "        \"Robust statistical analysis (Monte Carlo with 10,000 iterations)\",\n",
      "        \"Very low p-values (<0.001) across all decades\",\n",
      "        \"Clear acceleration trend matching GHG emission patterns\",\n",
      "        \"Uncertainty measurements included for all values\"\n",
      "      ],\n",
      "      \"key_limitations\": [\n",
      "        \"Focus on Arctic region only - may not represent global patterns\",\n",
      "        \"Relatively recent data (post-1979) compared to full climate record\",\n",
      "        \"Potential satellite calibration issues across different instruments\",\n",
      "        \"Does not directly measure GHG concentrations\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"numerical_values\": [\n",
      "      {\n",
      "        \"type\": \"correlation\",\n",
      "        \"value\": 0.92,\n",
      "        \"p_value\": \"<0.001\",\n",
      "        \"description\": \"Statistical correlation between CO2 levels and temperature\"\n",
      "      },\n",
      "      {\n",
      "        \"type\": \"r_squared\",\n",
      "        \"value\": 0.846,\n",
      "        \"p_value\": \"<0.001\",\n",
      "        \"description\": \"Variance in temperature explained by CO2 emissions\"\n",
      "      },\n",
      "      {\n",
      "        \"type\": \"regression_coefficient\",\n",
      "        \"value\": 0.792,\n",
      "        \"std_error\": 0.034,\n",
      "        \"p_value\": \"<0.001\",\n",
      "        \"description\": \"CO2 emissions coefficient in multiple regression\"\n",
      "      },\n",
      "      {\n",
      "        \"type\": \"emissions_increase\",\n",
      "        \"value\": 167,\n",
      "        \"unit\": \"percent\",\n",
      "        \"period\": \"since 1965\",\n",
      "        \"description\": \"Increase in annual global CO2 emissions\"\n",
      "      },\n",
      "      {\n",
      "        \"type\": \"temperature_increase\",\n",
      "        \"value\": 0.98,\n",
      "        \"unit\": \"celsius\",\n",
      "        \"period\": \"since pre-industrial\",\n",
      "        \"description\": \"Global temperature anomaly increase\"\n",
      "      }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "      \"data_sources\": [\n",
      "        \"Global Carbon Project\",\n",
      "        \"NASA GISS temperature records\",\n",
      "        \"World Meteorological Organization\"\n",
      "      ],\n",
      "      \"analysis_tools\": \"R version 4.2.1 with tidyverse and stats packages\",\n",
      "      \"publication_date\": \"2023-12-10\",\n",
      "      \"methodology\": \"Multiple regression controlling for natural variability factors\"\n",
      "    },\n",
      "    \"issues\": [\n",
      "      \"Time period for temperature increase (pre-industrial) differs from emissions data (since 1965)\",\n",
      "      \"Natural variability factors show significant effects (solar activity p=0.003, volcanic activity p=0.018)\",\n",
      "      \"Emissions data grouped by decade may mask year-to-year variations\"\n",
      "    ],\n",
      "    \"confidence_assessment\": {\n",
      "      \"confidence_score\": 0.85,\n",
      "      \"explanation\": \"The data shows strong statistical evidence for human-caused warming through multiple analytical approaches, with high statistical significance and strong effect sizes. The analysis accounts for major natural variables and uses reputable data sources.\",\n",
      "      \"key_strengths\": [\n",
      "        \"High statistical significance (p<0.001) across multiple measures\",\n",
      "        \"Strong correlation coefficient (r=0.92) and R-squared value (0.846)\",\n",
      "        \"Analysis controls for natural variability factors\",\n",
      "        \"Data from authoritative sources\",\n",
      "        \"Multiple statistical approaches showing consistent results\"\n",
      "      ],\n",
      "      \"key_limitations\": [\n",
      "        \"Temporal mismatch between some measurements\",\n",
      "        \"Decade-level aggregation may obscure finer patterns\",\n",
      "        \"Natural factors still show significant effects\",\n",
      "        \"Limited data before 1965\",\n",
      "        \"Potential confounding variables not included in the analysis\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "            \n",
      "            Is the new data redundant with any existing data points?\n",
      "            Consider:\n",
      "            1. Same source or study being cited\n",
      "            2. Same measurements within uncertainty\n",
      "            3. Derived results from already incorporated primary data\n",
      "            \n",
      "            Return \"true\" if redundant, \"false\" if novel information.\n",
      "            \n",
      "DEBUG:__main__:Redundancy check response: After analyzing the data, I conclude: false\n",
      "\n",
      "Reasoning:\n",
      "1. Different sources/studies:\n",
      "- The new data appears to be a meta-analysis of 147 studies focusing on climate sensitivity and future projections\n",
      "- The existing data points are from:\n",
      "  a) Arctic Climate Research Institute focusing on Arctic warming rates\n",
      "  b) A statistical analysis of CO2-temperature correlations using Global Carbon Project/NASA data\n",
      "\n",
      "2. Different measurements:\n",
      "- The new data focuses on:\n",
      "  - Equilibrium climate sensitivity (3.4C per CO2 doubling)\n",
      "  - Projected warming scenarios for 2100\n",
      "  - Uncertainty reduction in climate sensitivity estimates\n",
      "- The existing data covers:\n",
      "  - Arctic-specific warming rates\n",
      "  - CO2-temperature correlations and regression analyses\n",
      "  - Historical emissions and temperature increases\n",
      "\n",
      "3. Independent primary data:\n",
      "- The new meta-analysis provides novel synthesis of multiple studies\n",
      "- The uncertainty ranges and projections presented are unique\n",
      "- The methodology (Bayesian hierarchical model) differs from the statistical approaches in existing data\n",
      "- The time periods and focus areas are different\n",
      "\n",
      "While there may be some overlap in the broader climate change context, the new data provides novel information about climate sensitivity and future projections that isn't present in the existing dataset.\n",
      "INFO:__main__:Redundancy check result: False\n",
      "INFO:__main__:Analyzing parsed data\n",
      "DEBUG:__main__:Analysis prompt: \n",
      "            Given this data:\n",
      "            {\n",
      "  \"numerical_values\": [\n",
      "    {\n",
      "      \"metric\": \"equilibrium_climate_sensitivity_best_estimate\",\n",
      "      \"value\": 3.4,\n",
      "      \"units\": \"\\u00b0C per CO2 doubling\",\n",
      "      \"uncertainty_range\": [\n",
      "        2.8,\n",
      "        4.0\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_high_emissions\",\n",
      "      \"value\": 4.7,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        4.0,\n",
      "        5.4\n",
      "      ],\n",
      "      \"co2_level\": 850\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"projected_warming_2100_low_emissions\",\n",
      "      \"value\": 1.8,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"uncertainty_range\": [\n",
      "        1.5,\n",
      "        2.1\n",
      "      ],\n",
      "      \"co2_level\": 450\n",
      "    },\n",
      "    {\n",
      "      \"metric\": \"uncertainty_reduction\",\n",
      "      \"value\": 1.3,\n",
      "      \"units\": \"\\u00b0C\",\n",
      "      \"description\": \"Reduction in ECS uncertainty range from 2000-2004 (2.5\\u00b0C) to 2020-2023 (1.2\\u00b0C)\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"study_count\": 147,\n",
      "    \"time_period\": \"2000-2023\",\n",
      "    \"study_types\": [\n",
      "      \"Instrumental Record\",\n",
      "      \"Paleoclimate\",\n",
      "      \"Climate Models\",\n",
      "      \"Process Studies\"\n",
      "    ],\n",
      "    \"quality_scores\": {\n",
      "      \"instrumental\": 8.4,\n",
      "      \"paleoclimate\": 7.9,\n",
      "      \"climate_models\": 8.7,\n",
      "      \"process_studies\": 8.2\n",
      "    },\n",
      "    \"methodology\": \"Bayesian hierarchical model meta-analysis\",\n",
      "    \"peer_reviewed\": true\n",
      "  },\n",
      "  \"issues\": [\n",
      "    \"Potential publication bias in study selection\",\n",
      "    \"Varying methodologies across studies may affect comparability\",\n",
      "    \"Historical data limitations in paleoclimate studies\",\n",
      "    \"Model-based projections inherit uncertainties from underlying assumptions\"\n",
      "  ],\n",
      "  \"confidence_assessment\": {\n",
      "    \"confidence_score\": 0.85,\n",
      "    \"explanation\": \"The meta-analysis provides strong evidence for human-induced warming through GHG emissions, supported by multiple independent lines of evidence showing consistent results. The convergence of estimates across different methodologies and narrowing uncertainty ranges over time strengthen the conclusions.\",\n",
      "    \"key_strengths\": [\n",
      "      \"Large sample size (147 peer-reviewed studies)\",\n",
      "      \"Multiple independent methodologies showing consistent results\",\n",
      "      \"Rigorous statistical analysis using Bayesian hierarchical modeling\",\n",
      "      \"Significant narrowing of uncertainty ranges over time\",\n",
      "      \"High quality scores across study types (7.9-8.7/10)\",\n",
      "      \"Clear quantification of uncertainties and confidence intervals\"\n",
      "    ],\n",
      "    \"key_limitations\": [\n",
      "      \"Remaining uncertainty in exact sensitivity values\",\n",
      "      \"Complex interactions in climate system may not be fully captured\",\n",
      "      \"Future projections depend on emission scenario assumptions\",\n",
      "      \"Potential systematic biases in historical data reconstruction\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "            \n",
      "            Generate Python code to calculate log likelihoods for the hypothesis:\n",
      "            \"Human-generated GHG emissions significantly increase global temperatures\"\n",
      "\n",
      "            This should be a single function named `calculate_log_likelihoods`.\n",
      "            It should take a single argument, a dict with the format given above,\n",
      "            and output only the tuple of log-likelihoods\n",
      "              l_plus = log P(data | hypothesis),\n",
      "              l_minus = log P(data | not hypothesis).\n",
      "            \n",
      "            The code should:\n",
      "            1. Calculate l_plus and l_minus (log likelihoods)\n",
      "            2. Handle uncertainties properly\n",
      "            3. Account for data quality and potential biases\n",
      "            4. Limit overconfidence by capping the absolute difference between l_plus and l_minus to 5.0.\n",
      "            4. Use the usual libraries such as numpy and scipy for calculations\n",
      "            5. Use print() to output intermediate results, as well as the final result before returning.\n",
      "            \n",
      "            Return only executable Python code with the function definition.\n",
      "            Do not include the function call itself.\n",
      "            \n",
      "DEBUG:__main__:API response: ```python\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key values and uncertainties\n",
      "    ecs = data['numerical_values'][0]\n",
      "    high_warming = data['numerical_values'][1] \n",
      "    low_warming = data['numerical_values'][2]\n",
      "    \n",
      "    # Calculate likelihood components\n",
      "    \n",
      "    # 1. ECS likelihood based on uncertainty range\n",
      "    ecs_std = (ecs['uncertainty_range'][1] - ecs['uncertainty_range'][0])/4\n",
      "    ecs_likelihood = stats.norm.pdf(ecs['value'], loc=0, scale=ecs_std)\n",
      "    print(f\"ECS likelihood: {ecs_likelihood}\")\n",
      "    \n",
      "    # 2. Warming projections likelihood\n",
      "    high_std = (high_warming['uncertainty_range'][1] - high_warming['uncertainty_range'][0])/4\n",
      "    low_std = (low_warming['uncertainty_range'][1] - low_warming['uncertainty_range'][0])/4\n",
      "    \n",
      "    warming_likelihood = stats.norm.pdf(high_warming['value'], loc=low_warming['value'], scale=np.sqrt(high_std**2 + low_std**2))\n",
      "    print(f\"Warming likelihood: {warming_likelihood}\")\n",
      "    \n",
      "    # 3. Quality score adjustment\n",
      "    quality_scores = data['metadata']['quality_scores']\n",
      "    avg_quality = np.mean(list(quality_scores.values()))/10\n",
      "    print(f\"Average quality score: {avg_quality}\")\n",
      "    \n",
      "    # 4. Confidence adjustment\n",
      "    confidence = data['confidence_assessment']['confidence_score']\n",
      "    print(f\"Confidence score: {confidence}\")\n",
      "    \n",
      "    # Calculate log likelihoods\n",
      "    l_plus = np.log(ecs_likelihood * warming_likelihood * avg_quality * confidence)\n",
      "    \n",
      "    # Null hypothesis likelihood - assume natural variation only\n",
      "    l_minus = np.log((1-ecs_likelihood) * (1-warming_likelihood) * (1-avg_quality) * (1-confidence))\n",
      "    \n",
      "    # Cap difference to avoid overconfidence\n",
      "    diff = abs(l_plus - l_minus)\n",
      "    if diff > 5.0:\n",
      "        scale = 5.0/diff\n",
      "        l_plus *= scale\n",
      "        l_minus *= scale\n",
      "        \n",
      "    print(f\"Final l_plus: {l_plus}\")\n",
      "    print(f\"Final l_minus: {l_minus}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "```\n",
      "DEBUG:__main__:Extracted Python code: import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key values and uncertainties\n",
      "    ecs = data['numerical_values'][0]\n",
      "    high_warming = data['numerical_values'][1] \n",
      "    low_warming = data['numerical_values'][2]\n",
      "    \n",
      "    # Calculate likelihood components\n",
      "    \n",
      "    # 1. ECS likelihood based on uncertainty range\n",
      "    ecs_std = (ecs['uncertainty_range'][1] - ecs['uncertainty_range'][0])/4\n",
      "    ecs_likelihood = stats.norm.pdf(ecs['value'], loc=0, scale=ecs_std)\n",
      "    print(f\"ECS likelihood: {ecs_likelihood}\")\n",
      "    \n",
      "    # 2. Warming projections likelihood\n",
      "    high_std = (high_warming['uncertainty_range'][1] - high_warming['uncertainty_range'][0])/4\n",
      "    low_std = (low_warming['uncertainty_range'][1] - low_warming['uncertainty_range'][0])/4\n",
      "    \n",
      "    warming_likelihood = stats.norm.pdf(high_warming['value'], loc=low_warming['value'], scale=np.sqrt(high_std**2 + low_std**2))\n",
      "    print(f\"Warming likelihood: {warming_likelihood}\")\n",
      "    \n",
      "    # 3. Quality score adjustment\n",
      "    quality_scores = data['metadata']['quality_scores']\n",
      "    avg_quality = np.mean(list(quality_scores.values()))/10\n",
      "    print(f\"Average quality score: {avg_quality}\")\n",
      "    \n",
      "    # 4. Confidence adjustment\n",
      "    confidence = data['confidence_assessment']['confidence_score']\n",
      "    print(f\"Confidence score: {confidence}\")\n",
      "    \n",
      "    # Calculate log likelihoods\n",
      "    l_plus = np.log(ecs_likelihood * warming_likelihood * avg_quality * confidence)\n",
      "    \n",
      "    # Null hypothesis likelihood - assume natural variation only\n",
      "    l_minus = np.log((1-ecs_likelihood) * (1-warming_likelihood) * (1-avg_quality) * (1-confidence))\n",
      "    \n",
      "    # Cap difference to avoid overconfidence\n",
      "    diff = abs(l_plus - l_minus)\n",
      "    if diff > 5.0:\n",
      "        scale = 5.0/diff\n",
      "        l_plus *= scale\n",
      "        l_minus *= scale\n",
      "        \n",
      "    print(f\"Final l_plus: {l_plus}\")\n",
      "    print(f\"Final l_minus: {l_minus}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "INFO:__main__:Code execution attempt 1/5\n",
      "DEBUG:__main__:Executing code:\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def calculate_log_likelihoods(data):\n",
      "    # Extract key values and uncertainties\n",
      "    ecs = data['numerical_values'][0]\n",
      "    high_warming = data['numerical_values'][1] \n",
      "    low_warming = data['numerical_values'][2]\n",
      "    \n",
      "    # Calculate likelihood components\n",
      "    \n",
      "    # 1. ECS likelihood based on uncertainty range\n",
      "    ecs_std = (ecs['uncertainty_range'][1] - ecs['uncertainty_range'][0])/4\n",
      "    ecs_likelihood = stats.norm.pdf(ecs['value'], loc=0, scale=ecs_std)\n",
      "    print(f\"ECS likelihood: {ecs_likelihood}\")\n",
      "    \n",
      "    # 2. Warming projections likelihood\n",
      "    high_std = (high_warming['uncertainty_range'][1] - high_warming['uncertainty_range'][0])/4\n",
      "    low_std = (low_warming['uncertainty_range'][1] - low_warming['uncertainty_range'][0])/4\n",
      "    \n",
      "    warming_likelihood = stats.norm.pdf(high_warming['value'], loc=low_warming['value'], scale=np.sqrt(high_std**2 + low_std**2))\n",
      "    print(f\"Warming likelihood: {warming_likelihood}\")\n",
      "    \n",
      "    # 3. Quality score adjustment\n",
      "    quality_scores = data['metadata']['quality_scores']\n",
      "    avg_quality = np.mean(list(quality_scores.values()))/10\n",
      "    print(f\"Average quality score: {avg_quality}\")\n",
      "    \n",
      "    # 4. Confidence adjustment\n",
      "    confidence = data['confidence_assessment']['confidence_score']\n",
      "    print(f\"Confidence score: {confidence}\")\n",
      "    \n",
      "    # Calculate log likelihoods\n",
      "    l_plus = np.log(ecs_likelihood * warming_likelihood * avg_quality * confidence)\n",
      "    \n",
      "    # Null hypothesis likelihood - assume natural variation only\n",
      "    l_minus = np.log((1-ecs_likelihood) * (1-warming_likelihood) * (1-avg_quality) * (1-confidence))\n",
      "    \n",
      "    # Cap difference to avoid overconfidence\n",
      "    diff = abs(l_plus - l_minus)\n",
      "    if diff > 5.0:\n",
      "        scale = 5.0/diff\n",
      "        l_plus *= scale\n",
      "        l_minus *= scale\n",
      "        \n",
      "    print(f\"Final l_plus: {l_plus}\")\n",
      "    print(f\"Final l_minus: {l_minus}\")\n",
      "    \n",
      "    return (l_plus, l_minus)\n",
      "INFO:__main__:Code execution successful - l_plus: -5.204815281782804, l_minus: -0.20481528178280323\n",
      "INFO:__main__:Analysis results - l_plus: -5.204815281782804, l_minus: -0.20481528178280323\n",
      "INFO:__main__:Updated posterior from 10.0 to 4.999999999999999\n",
      "INFO:__main__:Extracting metadata from /content/drive/MyDrive/InFact Prototype/evidence/meta-analysis.html\n",
      "DEBUG:__main__:Extracted metadata: {\n",
      "  \"filename\": \"meta-analysis.html\",\n",
      "  \"file_type\": \".html\",\n",
      "  \"file_size\": 9552,\n",
      "  \"last_modified\": \"2025-01-04T18:25:40\",\n",
      "  \"source_path\": \"/content/drive/MyDrive/InFact Prototype/evidence/meta-analysis.html\",\n",
      "  \"title\": \"Meta-Analysis of Climate Sensitivity Studies: 2000-2023\",\n",
      "  \"meta_tags\": {\n",
      "    \"viewport\": \"width=device-width, initial-scale=1.0\",\n",
      "    \"author\": \"International Climate Research Consortium\",\n",
      "    \"description\": \"Comprehensive meta-analysis of 147 climate sensitivity studies from 2000-2023\",\n",
      "    \"keywords\": \"climate sensitivity, meta-analysis, global warming, climate models\",\n",
      "    \"publication-date\": \"2024-01-02\",\n",
      "    \"og:type\": \"article\",\n",
      "    \"og:site_name\": \"International Climate Research Consortium\"\n",
      "  },\n",
      "  \"has_article\": false,\n",
      "  \"has_main\": true,\n",
      "  \"num_headers\": 13,\n",
      "  \"has_tables\": true\n",
      "}\n",
      "INFO:__main__:Processing complete. Current probability: 99.33% (98.14%, 99.92%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ECS likelihood: 1.7077806340693132e-28\n",
      "Warming likelihood: 2.6649317405792824e-13\n",
      "Average quality score: 0.8300000000000001\n",
      "Confidence score: 0.85\n",
      "Final l_plus: -5.204815281782804\n",
      "Final l_minus: -0.20481528178280323\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4.999999999999999, (0.9813645501788257, 0.9991985884489322))"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "node.save(\"/content/drive/MyDrive/InFact Prototype/results/analysis.json\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "collapsed": true,
    "id": "fU8KycGvX3Bf",
    "outputId": "46cb1924-c794-41b8-a20c-d7bfffb78dd9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:__main__:Saving node data to /content/drive/MyDrive/InFact Prototype/results/analysis.json\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "from IPython.display import HTML\n",
    "# Render visualization\n",
    "renderer = InFactRenderer()\n",
    "output_file = \"/content/drive/MyDrive/InFact Prototype/results/analysis.html\"\n",
    "html = renderer.render_analysis(node, output_file)\n",
    "display(HTML(html))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "cellView": "form",
    "id": "mVHxK_oXm2Bn",
    "outputId": "3d92d035-fe33-4e62-ba20-620b3d5d403d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>InFact Analysis: Human-generated GHG emissions significantly increase global temperatures</title>\n",
       "    <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: system-ui, -apple-system, sans-serif;\n",
       "            line-height: 1.5;\n",
       "            max-width: 1200px;\n",
       "            margin: 0 auto;\n",
       "            padding: 2rem;\n",
       "            color: #1a1a1a;\n",
       "        }\n",
       "        \n",
       "        .card {\n",
       "            background: white;\n",
       "            border-radius: 8px;\n",
       "            box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
       "            padding: 1.5rem;\n",
       "            margin-bottom: 1.5rem;\n",
       "        }\n",
       "        \n",
       "        .hypothesis {\n",
       "            font-size: 1.2rem;\n",
       "            color: #4a5568;\n",
       "            margin: 1rem 0;\n",
       "            padding: 1rem;\n",
       "            background: #f7fafc;\n",
       "            border-radius: 6px;\n",
       "        }\n",
       "        \n",
       "        .final-assessment {\n",
       "            text-align: center;\n",
       "            padding: 2rem;\n",
       "            background: #ebf8ff;\n",
       "        }\n",
       "        \n",
       "        .probability {\n",
       "            font-size: 3rem;\n",
       "            font-weight: bold;\n",
       "            color: #2b6cb0;\n",
       "        }\n",
       "        \n",
       "        .uncertainty {\n",
       "            font-size: 1.2rem;\n",
       "            color: #4a5568;\n",
       "            margin-top: 0.5rem;\n",
       "        }\n",
       "        \n",
       "        .evidence-point {\n",
       "            border-left: 4px solid #4299e1;\n",
       "            padding-left: 1rem;\n",
       "            margin-bottom: 2rem;\n",
       "        }\n",
       "        \n",
       "        .evidence-grid {\n",
       "            display: grid;\n",
       "            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n",
       "            gap: 1rem;\n",
       "        }\n",
       "        \n",
       "        .stats-grid {\n",
       "            display: grid;\n",
       "            grid-template-columns: repeat(3, 1fr);\n",
       "            gap: 1rem;\n",
       "            margin: 1rem 0;\n",
       "        }\n",
       "        \n",
       "        .stat-card {\n",
       "            background: #f7fafc;\n",
       "            padding: 1rem;\n",
       "            border-radius: 6px;\n",
       "            text-align: center;\n",
       "        }\n",
       "        \n",
       "        .stat-value {\n",
       "            font-size: 1.5rem;\n",
       "            font-weight: bold;\n",
       "            color: #2b6cb0;\n",
       "        }\n",
       "        \n",
       "        .stat-label {\n",
       "            font-size: 0.875rem;\n",
       "            color: #4a5568;\n",
       "            margin-top: 0.25rem;\n",
       "        }\n",
       "        \n",
       "        .chart-container {\n",
       "            height: 400px;\n",
       "            margin: 2rem 0;\n",
       "        }\n",
       "        \n",
       "        .confidence-high { color: #047857; }\n",
       "        .confidence-medium { color: #b45309; }\n",
       "        .confidence-low { color: #dc2626; }\n",
       "        \n",
       "        details {\n",
       "            margin-top: 1rem;\n",
       "        }\n",
       "        \n",
       "        summary {\n",
       "            cursor: pointer;\n",
       "            color: #2b6cb0;\n",
       "            font-weight: 500;\n",
       "        }\n",
       "        \n",
       "        pre {\n",
       "            background: #f7fafc;\n",
       "            padding: 1rem;\n",
       "            border-radius: 6px;\n",
       "            overflow-x: auto;\n",
       "            font-size: 0.875rem;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "    <header>\n",
       "        <h1>Evidence Analysis</h1>\n",
       "        <div class=\"hypothesis\">Human-generated GHG emissions significantly increase global temperatures</div>\n",
       "    </header>\n",
       "\n",
       "    <main>\n",
       "        <section class=\"card final-assessment\">\n",
       "            <h2>Current Assessment</h2>\n",
       "            <div class=\"probability\">\n",
       "                99.3%\n",
       "            </div>\n",
       "            <div class=\"uncertainty\">\n",
       "                (98.1%, 99.9%)\n",
       "            </div>\n",
       "            <div class=\"interpretation\">\n",
       "                Virtually Certain\n",
       "                \n",
       "            </div>\n",
       "        </section>\n",
       "\n",
       "        <section class=\"card\">\n",
       "            <h2>Belief Evolution</h2>\n",
       "            <div class=\"chart-container\">\n",
       "                <canvas id=\"beliefChart\"></canvas>\n",
       "            </div>\n",
       "        </section>\n",
       "\n",
       "        <section class=\"card\">\n",
       "            <h2>Evidence Analysis</h2>\n",
       "            \n",
       "            <div class=\"evidence-point\">\n",
       "                <h3>Evidence 1: climate-study.html</h3>\n",
       "                \n",
       "                <div class=\"stats-grid\">\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">50.0%</div>\n",
       "                        <div class=\"stat-label\">Prior Probability</div>\n",
       "                    </div>\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">148.4</div>\n",
       "                        <div class=\"stat-label\">Likelihood Ratio</div>\n",
       "                    </div>\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">99.3%</div>\n",
       "                        <div class=\"stat-label\">Posterior Probability</div>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"evidence-grid\">\n",
       "                    <div>\n",
       "                        <h4>Confidence Assessment</h4>\n",
       "                        <div class=\"confidence-score \n",
       "                            confidence-high\n",
       "                            \">\n",
       "                            85% Confidence\n",
       "                        </div>\n",
       "                        \n",
       "                        \n",
       "                        <h5>Key Strengths</h5>\n",
       "                        <ul>\n",
       "                            \n",
       "                            <li>Long-term continuous satellite data (44 years)</li>\n",
       "                            \n",
       "                            <li>Cross-validation with ground stations</li>\n",
       "                            \n",
       "                            <li>Robust statistical analysis (Monte Carlo with 10,000 iterations)</li>\n",
       "                            \n",
       "                            <li>Very low p-values (<0.001) across all decades</li>\n",
       "                            \n",
       "                            <li>Clear acceleration trend matching GHG emission patterns</li>\n",
       "                            \n",
       "                            <li>Uncertainty measurements included for all values</li>\n",
       "                            \n",
       "                        </ul>\n",
       "                        \n",
       "                        \n",
       "                        \n",
       "                        <h5>Key Limitations</h5>\n",
       "                        <ul>\n",
       "                            \n",
       "                            <li>Focus on Arctic region only - may not represent global patterns</li>\n",
       "                            \n",
       "                            <li>Relatively recent data (post-1979) compared to full climate record</li>\n",
       "                            \n",
       "                            <li>Potential satellite calibration issues across different instruments</li>\n",
       "                            \n",
       "                            <li>Does not directly measure GHG concentrations</li>\n",
       "                            \n",
       "                        </ul>\n",
       "                        \n",
       "                    </div>\n",
       "                </div>\n",
       "\n",
       "                <details>\n",
       "                    <summary>Analysis Details</summary>\n",
       "                    <div class=\"rationale\">\n",
       "                        \n",
       "                        <pre><code>import numpy as np\n",
       "from scipy import stats\n",
       "\n",
       "def calculate_log_likelihoods(data):\n",
       "    # Extract key values\n",
       "    warming_rate = data[\"numerical_values\"][0][\"value\"]\n",
       "    early_anomaly = data[\"numerical_values\"][1][\"anomaly\"] \n",
       "    early_uncertainty = data[\"numerical_values\"][1][\"uncertainty\"]\n",
       "    recent_anomaly = data[\"numerical_values\"][2][\"anomaly\"]\n",
       "    recent_uncertainty = data[\"numerical_values\"][2][\"uncertainty\"]\n",
       "    confidence = data[\"confidence_assessment\"][\"confidence_score\"]\n",
       "    \n",
       "    # Calculate temperature change significance\n",
       "    z_score = (recent_anomaly - early_anomaly) / np.sqrt(early_uncertainty**2 + recent_uncertainty**2)\n",
       "    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
       "    print(f\"Temperature change z-score: {z_score:.2f}, p-value: {p_value:.4f}\")\n",
       "\n",
       "    # Calculate base likelihoods\n",
       "    # Higher likelihood under hypothesis if:\n",
       "    # - Large warming rate\n",
       "    # - Strong recent anomaly\n",
       "    # - Low p-value\n",
       "    # - High confidence score\n",
       "    l_plus_base = (\n",
       "        np.log(warming_rate) +\n",
       "        np.log(recent_anomaly + 1) + \n",
       "        -np.log(p_value + 1e-10) +\n",
       "        np.log(confidence)\n",
       "    )\n",
       "    \n",
       "    # Lower likelihood under null hypothesis\n",
       "    l_minus_base = (\n",
       "        -np.log(warming_rate) +\n",
       "        -np.log(recent_anomaly + 1) +\n",
       "        np.log(p_value + 1e-10) +\n",
       "        -np.log(confidence)\n",
       "    )\n",
       "    \n",
       "    print(f\"Base log likelihoods - l_plus: {l_plus_base:.2f}, l_minus: {l_minus_base:.2f}\")\n",
       "\n",
       "    # Adjust for data quality issues\n",
       "    quality_penalty = len(data[\"issues\"]) * 0.2\n",
       "    l_plus = l_plus_base - quality_penalty\n",
       "    l_minus = l_minus_base + quality_penalty\n",
       "    \n",
       "    print(f\"Quality-adjusted log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
       "\n",
       "    # Cap the absolute difference to avoid overconfidence\n",
       "    diff = l_plus - l_minus\n",
       "    if abs(diff) > 5.0:\n",
       "        adjustment = (abs(diff) - 5.0) / 2\n",
       "        if diff > 0:\n",
       "            l_plus -= adjustment\n",
       "            l_minus += adjustment\n",
       "        else:\n",
       "            l_plus += adjustment \n",
       "            l_minus -= adjustment\n",
       "            \n",
       "    print(f\"Final capped log likelihoods - l_plus: {l_plus:.2f}, l_minus: {l_minus:.2f}\")\n",
       "    \n",
       "    return (l_plus, l_minus)</code></pre>\n",
       "                        \n",
       "                    </div>\n",
       "                </details>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"evidence-point\">\n",
       "                <h3>Evidence 2: emissions-analysis.html</h3>\n",
       "                \n",
       "                <div class=\"stats-grid\">\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">99.3%</div>\n",
       "                        <div class=\"stat-label\">Prior Probability</div>\n",
       "                    </div>\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">148.4</div>\n",
       "                        <div class=\"stat-label\">Likelihood Ratio</div>\n",
       "                    </div>\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">100.0%</div>\n",
       "                        <div class=\"stat-label\">Posterior Probability</div>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"evidence-grid\">\n",
       "                    <div>\n",
       "                        <h4>Confidence Assessment</h4>\n",
       "                        <div class=\"confidence-score \n",
       "                            confidence-high\n",
       "                            \">\n",
       "                            85% Confidence\n",
       "                        </div>\n",
       "                        \n",
       "                        \n",
       "                        <h5>Key Strengths</h5>\n",
       "                        <ul>\n",
       "                            \n",
       "                            <li>High statistical significance (p<0.001) across multiple measures</li>\n",
       "                            \n",
       "                            <li>Strong correlation coefficient (r=0.92) and R-squared value (0.846)</li>\n",
       "                            \n",
       "                            <li>Analysis controls for natural variability factors</li>\n",
       "                            \n",
       "                            <li>Data from authoritative sources</li>\n",
       "                            \n",
       "                            <li>Multiple statistical approaches showing consistent results</li>\n",
       "                            \n",
       "                        </ul>\n",
       "                        \n",
       "                        \n",
       "                        \n",
       "                        <h5>Key Limitations</h5>\n",
       "                        <ul>\n",
       "                            \n",
       "                            <li>Temporal mismatch between some measurements</li>\n",
       "                            \n",
       "                            <li>Decade-level aggregation may obscure finer patterns</li>\n",
       "                            \n",
       "                            <li>Natural factors still show significant effects</li>\n",
       "                            \n",
       "                            <li>Limited data before 1965</li>\n",
       "                            \n",
       "                            <li>Potential confounding variables not included in the analysis</li>\n",
       "                            \n",
       "                        </ul>\n",
       "                        \n",
       "                    </div>\n",
       "                </div>\n",
       "\n",
       "                <details>\n",
       "                    <summary>Analysis Details</summary>\n",
       "                    <div class=\"rationale\">\n",
       "                        \n",
       "                        <pre><code>import numpy as np\n",
       "from scipy import stats\n",
       "\n",
       "def calculate_log_likelihoods(data):\n",
       "    # Extract key statistical measures\n",
       "    correlation = data['numerical_values'][0]['value']\n",
       "    r_squared = data['numerical_values'][1]['value'] \n",
       "    regression_coef = data['numerical_values'][2]['value']\n",
       "    regression_se = data['numerical_values'][2]['std_error']\n",
       "    \n",
       "    # Calculate components for likelihood under H1 (hypothesis true)\n",
       "    \n",
       "    # 1. Correlation strength component\n",
       "    corr_component = stats.norm.logpdf(correlation, loc=0.8, scale=0.1)\n",
       "    print(f\"Correlation component: {corr_component:.3f}\")\n",
       "    \n",
       "    # 2. R-squared component \n",
       "    r2_component = stats.beta.logpdf(r_squared, a=20, b=4)\n",
       "    print(f\"R-squared component: {r2_component:.3f}\")\n",
       "    \n",
       "    # 3. Regression coefficient component\n",
       "    reg_z = regression_coef / regression_se\n",
       "    reg_component = stats.norm.logpdf(reg_z, loc=20, scale=5)\n",
       "    print(f\"Regression component: {reg_component:.3f}\")\n",
       "    \n",
       "    # 4. Data quality adjustment\n",
       "    confidence_score = data['confidence_assessment']['confidence_score']\n",
       "    quality_adjustment = np.log(confidence_score)\n",
       "    print(f\"Quality adjustment: {quality_adjustment:.3f}\")\n",
       "    \n",
       "    # Calculate l_plus (log likelihood under H1)\n",
       "    l_plus = corr_component + r2_component + reg_component + quality_adjustment\n",
       "    \n",
       "    # Calculate l_minus (log likelihood under H0)\n",
       "    # Assume weaker relationships under null hypothesis\n",
       "    null_corr = stats.norm.logpdf(correlation, loc=0.2, scale=0.2)\n",
       "    null_r2 = stats.beta.logpdf(r_squared, a=2, b=10)\n",
       "    null_reg = stats.norm.logpdf(reg_z, loc=0, scale=10)\n",
       "    \n",
       "    l_minus = null_corr + null_r2 + null_reg + quality_adjustment\n",
       "    \n",
       "    # Apply uncertainty penalties\n",
       "    n_limitations = len(data['confidence_assessment']['key_limitations'])\n",
       "    uncertainty_penalty = 0.2 * n_limitations\n",
       "    l_plus -= uncertainty_penalty\n",
       "    l_minus -= uncertainty_penalty\n",
       "    \n",
       "    print(f\"Uncertainty penalty: {uncertainty_penalty:.3f}\")\n",
       "    \n",
       "    # Cap the absolute difference between l_plus and l_minus\n",
       "    diff = l_plus - l_minus\n",
       "    if abs(diff) > 5.0:\n",
       "        adjustment = abs(diff) - 5.0\n",
       "        if diff > 0:\n",
       "            l_plus -= adjustment/2\n",
       "            l_minus += adjustment/2\n",
       "        else:\n",
       "            l_plus += adjustment/2\n",
       "            l_minus -= adjustment/2\n",
       "            \n",
       "    print(f\"\\nFinal log likelihoods:\")\n",
       "    print(f\"l_plus: {l_plus:.3f}\")\n",
       "    print(f\"l_minus: {l_minus:.3f}\")\n",
       "    \n",
       "    return (l_plus, l_minus)</code></pre>\n",
       "                        \n",
       "                    </div>\n",
       "                </details>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"evidence-point\">\n",
       "                <h3>Evidence 3: meta-analysis.html</h3>\n",
       "                \n",
       "                <div class=\"stats-grid\">\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">100.0%</div>\n",
       "                        <div class=\"stat-label\">Prior Probability</div>\n",
       "                    </div>\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">0.0</div>\n",
       "                        <div class=\"stat-label\">Likelihood Ratio</div>\n",
       "                    </div>\n",
       "                    <div class=\"stat-card\">\n",
       "                        <div class=\"stat-value\">99.3%</div>\n",
       "                        <div class=\"stat-label\">Posterior Probability</div>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"evidence-grid\">\n",
       "                    <div>\n",
       "                        <h4>Confidence Assessment</h4>\n",
       "                        <div class=\"confidence-score \n",
       "                            confidence-high\n",
       "                            \">\n",
       "                            85% Confidence\n",
       "                        </div>\n",
       "                        \n",
       "                        \n",
       "                        <h5>Key Strengths</h5>\n",
       "                        <ul>\n",
       "                            \n",
       "                            <li>Large sample size (147 peer-reviewed studies)</li>\n",
       "                            \n",
       "                            <li>Multiple independent methodologies showing consistent results</li>\n",
       "                            \n",
       "                            <li>Rigorous statistical analysis using Bayesian hierarchical modeling</li>\n",
       "                            \n",
       "                            <li>Significant narrowing of uncertainty ranges over time</li>\n",
       "                            \n",
       "                            <li>High quality scores across study types (7.9-8.7/10)</li>\n",
       "                            \n",
       "                            <li>Clear quantification of uncertainties and confidence intervals</li>\n",
       "                            \n",
       "                        </ul>\n",
       "                        \n",
       "                        \n",
       "                        \n",
       "                        <h5>Key Limitations</h5>\n",
       "                        <ul>\n",
       "                            \n",
       "                            <li>Remaining uncertainty in exact sensitivity values</li>\n",
       "                            \n",
       "                            <li>Complex interactions in climate system may not be fully captured</li>\n",
       "                            \n",
       "                            <li>Future projections depend on emission scenario assumptions</li>\n",
       "                            \n",
       "                            <li>Potential systematic biases in historical data reconstruction</li>\n",
       "                            \n",
       "                        </ul>\n",
       "                        \n",
       "                    </div>\n",
       "                </div>\n",
       "\n",
       "                <details>\n",
       "                    <summary>Analysis Details</summary>\n",
       "                    <div class=\"rationale\">\n",
       "                        \n",
       "                        <pre><code>import numpy as np\n",
       "from scipy import stats\n",
       "\n",
       "def calculate_log_likelihoods(data):\n",
       "    # Extract key values and uncertainties\n",
       "    ecs = data['numerical_values'][0]\n",
       "    high_warming = data['numerical_values'][1] \n",
       "    low_warming = data['numerical_values'][2]\n",
       "    \n",
       "    # Calculate likelihood components\n",
       "    \n",
       "    # 1. ECS likelihood based on uncertainty range\n",
       "    ecs_std = (ecs['uncertainty_range'][1] - ecs['uncertainty_range'][0])/4\n",
       "    ecs_likelihood = stats.norm.pdf(ecs['value'], loc=0, scale=ecs_std)\n",
       "    print(f\"ECS likelihood: {ecs_likelihood}\")\n",
       "    \n",
       "    # 2. Warming projections likelihood\n",
       "    high_std = (high_warming['uncertainty_range'][1] - high_warming['uncertainty_range'][0])/4\n",
       "    low_std = (low_warming['uncertainty_range'][1] - low_warming['uncertainty_range'][0])/4\n",
       "    \n",
       "    warming_likelihood = stats.norm.pdf(high_warming['value'], loc=low_warming['value'], scale=np.sqrt(high_std**2 + low_std**2))\n",
       "    print(f\"Warming likelihood: {warming_likelihood}\")\n",
       "    \n",
       "    # 3. Quality score adjustment\n",
       "    quality_scores = data['metadata']['quality_scores']\n",
       "    avg_quality = np.mean(list(quality_scores.values()))/10\n",
       "    print(f\"Average quality score: {avg_quality}\")\n",
       "    \n",
       "    # 4. Confidence adjustment\n",
       "    confidence = data['confidence_assessment']['confidence_score']\n",
       "    print(f\"Confidence score: {confidence}\")\n",
       "    \n",
       "    # Calculate log likelihoods\n",
       "    l_plus = np.log(ecs_likelihood * warming_likelihood * avg_quality * confidence)\n",
       "    \n",
       "    # Null hypothesis likelihood - assume natural variation only\n",
       "    l_minus = np.log((1-ecs_likelihood) * (1-warming_likelihood) * (1-avg_quality) * (1-confidence))\n",
       "    \n",
       "    # Cap difference to avoid overconfidence\n",
       "    diff = abs(l_plus - l_minus)\n",
       "    if diff > 5.0:\n",
       "        scale = 5.0/diff\n",
       "        l_plus *= scale\n",
       "        l_minus *= scale\n",
       "        \n",
       "    print(f\"Final l_plus: {l_plus}\")\n",
       "    print(f\"Final l_minus: {l_minus}\")\n",
       "    \n",
       "    return (l_plus, l_minus)</code></pre>\n",
       "                        \n",
       "                    </div>\n",
       "                </details>\n",
       "            </div>\n",
       "            \n",
       "        </section>\n",
       "    </main>\n",
       "\n",
       "    <script>\n",
       "        const ctx = document.getElementById('beliefChart').getContext('2d');\n",
       "        new Chart(ctx, {\n",
       "            type: 'line',\n",
       "            data: {\n",
       "                labels: ['Prior'].concat([\"climate-study.html\", \"emissions-analysis.html\", \"meta-analysis.html\"]),\n",
       "                datasets: [{\n",
       "                    label: 'Belief Probability',\n",
       "                    data: [0.5].concat([0.9933071490757152, 0.9999546021312976, 0.9933071490757152]),\n",
       "                    borderColor: '#2b6cb0',\n",
       "                    backgroundColor: 'rgba(43, 108, 176, 0.1)',\n",
       "                    tension: 0.1\n",
       "                }]\n",
       "            },\n",
       "            options: {\n",
       "                responsive: true,\n",
       "                maintainAspectRatio: false,\n",
       "                scales: {\n",
       "                    y: {\n",
       "                        beginAtZero: true,\n",
       "                        max: 1,\n",
       "                        ticks: {\n",
       "                            callback: function(value) {\n",
       "                                return (value * 100) + '%';\n",
       "                            }\n",
       "                        }\n",
       "                    },\n",
       "                    x: {\n",
       "                        ticks: {\n",
       "                            maxRotation: 45,\n",
       "                            minRotation: 45\n",
       "                        }\n",
       "                    }\n",
       "                },\n",
       "                plugins: {\n",
       "                    tooltip: {\n",
       "                        callbacks: {\n",
       "                            label: function(context) {\n",
       "                                return (context.raw * 100).toFixed(1) + '%';\n",
       "                            }\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "        });\n",
       "    </script>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {}
    }
   ]
  }
 ]
}
